{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview:\n",
    "* Load metadata and tracking sheet details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests as req\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Authentication Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "config = ConfigParser()\n",
    "config.read(\"/Users/nathansuberi/Desktop/WRI_Programming/cred/.env\")\n",
    "api_token = config.get(\"auth\", \"rw_api_token\")\n",
    "\n",
    "auth_token = api_token # <Insert Auth Token Here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Download Google Spreadsheets ####\n",
    "# QC Ready Metadata\n",
    "!curl \"https://docs.google.com/spreadsheets/d/1UkABgMlBIinJjITa6WepFAL-8VBkulS0LCbKojRXjVY/export?format=tsv\" > current_metadata.tsv\n",
    "current_mdata = pd.read_csv(open(\"current_metadata.tsv\", \"r\"), sep=\"\\t\", index_col=[0])\n",
    "os.remove(\"current_metadata.tsv\")\n",
    "\n",
    "# Continue with the metadata that matches elements in the tracking sheet\n",
    "ids_on_backoffice = pd.notnull(current_mdata[\"final_ids\"])\n",
    "mdata_for_ids_on_backoffice = current_mdata.loc[ids_on_backoffice]\n",
    "\n",
    "# Should have used this:\n",
    "mdata_for_ids_on_backoffice = mdata_for_ids_on_backoffice.reset_index().set_index(\"final_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdata_for_ids_on_backoffice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_nulls(val):\n",
    "    \"\"\"Used to clean np.nan values from the metadata update call... which don't play nice with the RW API\"\"\"\n",
    "    try:\n",
    "        if np.isnan(val):\n",
    "            return(None)\n",
    "        else:\n",
    "            return(val)\n",
    "    except:\n",
    "        return(val)\n",
    "\n",
    "def create_source_object(sources):\n",
    "    \"\"\"Format the source information as appropriate for the api\"\"\"\n",
    "    if sources:\n",
    "        source_object = []\n",
    "        srcs = sources.split(\"/\")\n",
    "        for ix, src in enumerate(srcs):\n",
    "            source_object.append({\n",
    "                \"source-name\":src,\n",
    "                \"id\":ix,\n",
    "                \"source-description\":\"\"\n",
    "            })\n",
    "        return source_object\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## FOR EACH DATASET IN BACKOFFICE THAT HAS METADATA, UPLOAD IT\n",
    "\n",
    "### THIS ADDS ALL DATASETS FOR WHICH WE HAVE METADATA in METADATA FOR UPLOAD ###\n",
    "\n",
    "processed1 = []\n",
    "\n",
    "for rw_id in mdata_for_ids_on_backoffice.index:\n",
    "    url = \"https://api.resourcewatch.org/v1/dataset/\"+str(rw_id)+\"/metadata\"\n",
    "    metadata = mdata_for_ids_on_backoffice.loc[rw_id]\n",
    "    \n",
    "    # If there are multiple metadata entries for a single rw_id, print these for trouble-shooting\n",
    "    if len(metadata.shape) > 1:\n",
    "        print(metadata)\n",
    "\n",
    "    # If the data is of type raster, don't include the Download Data (S3) link\n",
    "    flag1 = clean_nulls(metadata[\"Data Type\"]) != None\n",
    "    if(flag1):\n",
    "        flag2 = clean_nulls(metadata[\"Data Type\"]).lower() != \"raster\"\n",
    "        if(flag2):\n",
    "            data_dl_link = clean_nulls(metadata[\"Download Data (S3)\"])\n",
    "        else:\n",
    "            data_dl_link = None\n",
    "    else:\n",
    "        data_dl_link = None\n",
    "\n",
    "    # If there is no download from source, default to the learn more link\n",
    "    if(clean_nulls(metadata[\"Download from Source\"]) != None):\n",
    "        data_dl_orig_link = clean_nulls(metadata[\"Download from Source\"])\n",
    "    else:\n",
    "        data_dl_orig_link = clean_nulls(metadata[\"Learn More Link\"])\n",
    "\n",
    "    # If there is no technical title, default to the public title\n",
    "    if(clean_nulls(metadata[\"Technical Title\"]) != None):\n",
    "        tech_title = clean_nulls(metadata[\"Technical Title\"])\n",
    "    else:\n",
    "        tech_title = clean_nulls(metadata[\"Public Title\"])\n",
    "\n",
    "    print(clean_nulls(metadata[\"Unique ID\"]))\n",
    "    row_payload = {\n",
    "        \"language\": \"en\",\n",
    "\n",
    "        \"name\": clean_nulls(metadata[\"Public Title\"]),\n",
    "        \"description\": clean_nulls(metadata[\"Description\"]),\n",
    "        \"subtitle\": clean_nulls(metadata[\"Subtitle\"]),\n",
    "        \"source\": clean_nulls(metadata[\"Subtitle\"]),\n",
    "        \"functions\": clean_nulls(metadata[\"Function\"]),\n",
    "\n",
    "        \"application\":\"rw\",\n",
    "        \"dataset\":rw_id,\n",
    "\n",
    "        \"info\": {\n",
    "            \"wri_rw_id\": clean_nulls(metadata[\"Unique ID\"]),\n",
    "            \"rwId\": clean_nulls(metadata[\"Unique ID\"]),\n",
    "\n",
    "            \"data_type\": clean_nulls(metadata[\"Data Type\"]),\n",
    "\n",
    "            \"name\": clean_nulls(metadata[\"Public Title\"]),\n",
    "            \"sources\": create_source_object(clean_nulls(metadata[\"Source Organizations\"])),\n",
    "\n",
    "            \"technical_title\":tech_title,\n",
    "\n",
    "            \"functions\": clean_nulls(metadata[\"Function\"]),\n",
    "            \"cautions\": clean_nulls(metadata[\"Cautions\"]),\n",
    "\n",
    "            \"citation\": clean_nulls(metadata[\"Citation\"]),\n",
    "\n",
    "            \"license\": clean_nulls(metadata[\"Summary of Licence\"]),\n",
    "            \"license_link\": clean_nulls(metadata[\"Link to License\"]),\n",
    "\n",
    "            \"geographic_coverage\": clean_nulls(metadata[\"Geographic Coverage\"]),\n",
    "            \"spatial_resolution\": clean_nulls(metadata[\"Spatial Resolution\"]),\n",
    "\n",
    "            \"date_of_content\": clean_nulls(metadata[\"Date of Content\"]),\n",
    "            \"frequency_of_updates\": clean_nulls(metadata[\"Frequency of Updates\"]),\n",
    "\n",
    "            \"learn_more_link\": clean_nulls(metadata[\"Learn More Link\"]),\n",
    "\n",
    "            \"data_download_link\": data_dl_link,\n",
    "            \"data_download_original_link\":data_dl_orig_link\n",
    "        }\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'content-type': \"application/json\",\n",
    "        'authorization': \"Bearer \" + auth_token,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        processed1.append(rw_id)\n",
    "        res = req.request(\"POST\", url, data=json.dumps(row_payload), headers = headers)\n",
    "        if res.ok:\n",
    "            print('New metadata uploaded')\n",
    "        else:\n",
    "            print('Whoops, already exists! Updating metadata.')\n",
    "            res = req.request(\"PATCH\", url, data=json.dumps(row_payload), headers = headers)\n",
    "            print('Ok now?:', res.ok)\n",
    "            if not res.ok:\n",
    "                print(res.text)\n",
    "                print(mdata_for_ids_on_backoffice.loc[rw_id])\n",
    "    except TypeError as e:\n",
    "        print(e.args)\n",
    "        print(metadata[[\"Unique ID\", \"Public Title\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# July Data Sheet\n",
    "!curl \"https://docs.google.com/spreadsheets/d/1viPOGYIk6RGu7YMoM3BHNVbkWaCZ0JFBOMSNncWvHYk/export?format=tsv\" > tracking_sheet.tsv\n",
    "tracking_sheet = pd.read_csv(\"tracking_sheet.tsv\", sep=\"\\t\", index_col=[0])\n",
    "os.remove(\"tracking_sheet.tsv\")\n",
    "\n",
    "# Metadata for Upload\n",
    "!curl \"https://docs.google.com/spreadsheets/d/1UkABgMlBIinJjITa6WepFAL-8VBkulS0LCbKojRXjVY/export?format=tsv\" > current_metadata.tsv\n",
    "current_mdata = pd.read_csv(open(\"current_metadata.tsv\", \"r\"), sep=\"\\t\", index_col=[0])\n",
    "os.remove(\"current_metadata.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From General Utilities\n",
    "\n",
    "def transfer_columns_between_tables(src_df, dst_df, columns_to_xfer=None):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    * src_df and dst_df with same index\n",
    "    * list of columns from src_df to transfer to dst_df\n",
    "    \n",
    "    Outputs:\n",
    "    * dst_df with new columns from the src_df\n",
    "    \"\"\"\n",
    "    \n",
    "    dst_df = dst_df.copy()\n",
    "    try:\n",
    "        info = src_df.loc[dst_df.index, columns_to_xfer]\n",
    "        dst_df[columns_to_xfer] = info\n",
    "        return(dst_df)\n",
    "    except:\n",
    "        print(\"ERROR\")\n",
    "        print(\"Possible source: There is an element in the metadata sheet that is not in the tracking sheet\")\n",
    "        print(\"Possible source: One of more of the given columns to transfer is not in the source df\")\n",
    "        return(None)\n",
    "    \n",
    "# Specific to this notebook\n",
    "\n",
    "def investigate_unmatched(src_df, dst_df):\n",
    "    \"\"\" This function assumes that the two dataframes share the same index of Unique IDs\"\"\"\n",
    "    ids = dst_df.index\n",
    "    unmatched = []\n",
    "    for ix, id in enumerate(ids):\n",
    "        try:\n",
    "            src_df.loc[id]\n",
    "        except:\n",
    "            unmatched.append((ix+2, id))\n",
    "    return(unmatched)\n",
    "\n",
    "def clean_nulls(val):\n",
    "    \"\"\"Used to clean np.nan values from the metadata update call... which don't play nice with the RW API\"\"\"\n",
    "    try:\n",
    "        if np.isnan(val):\n",
    "            return(None)\n",
    "        else:\n",
    "            return(val)\n",
    "    except:\n",
    "        return(val)\n",
    "\n",
    "def create_source_object(sources):\n",
    "    \"\"\"Format the source information as appropriate for the api\"\"\"\n",
    "    if sources:\n",
    "        source_object = []\n",
    "        srcs = sources.split(\"/\")\n",
    "        for ix, src in enumerate(srcs):\n",
    "            source_object.append({\n",
    "                \"source-name\":src,\n",
    "                \"id\":ix,\n",
    "                \"source-description\":\"\"\n",
    "            })\n",
    "        return source_object\n",
    "    return None\n",
    "\n",
    "def choose_new_id(df, old_id_col, new_id_col):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    * A dataframe with two columns - old_ids, and new_ids. Could be \"Perfect IDs\" vs. old ids on RW backoffice\n",
    "    * Names of the columns with the old_ids and new_ids\n",
    "    Outputs:\n",
    "    * A list of final_ids that can be appended to df\n",
    "    \"\"\"\n",
    "    final_ids = []\n",
    "    for i in range(df.shape[0]):\n",
    "        new_col = df.iloc[i][new_id_col]\n",
    "        old_col = df.iloc[i][old_id_col]\n",
    "        if(pd.notnull(new_col)):\n",
    "            final_ids.append(new_col)\n",
    "        elif(pd.notnull(old_col)):\n",
    "            final_ids.append(old_col)\n",
    "        else:\n",
    "            final_ids.append(None)\n",
    "    return(final_ids)\n",
    "\n",
    "# Sample usage\n",
    "old_list = [None, \"a\", None, \"b\"]\n",
    "new_list = [\"c\", None, None, \"d\"]\n",
    "df = pd.DataFrame({\"old\":old_list, \"new\":new_list})\n",
    "choose_new_id(df, \"old\", \"new\")\n",
    "\n",
    "#old_id_col = \"old_ids\"\n",
    "#new_id_col = \"new_ids\"\n",
    "#tracking_valid_old_ids = pd.notnull(tracking_sheet[old_id_col])\n",
    "#tracking_valid_new_ids = pd.notnull(tracking_sheet[new_id_col])\n",
    "#tracking_sheet[\"final_ids\"] = choose_new_id(tracking_sheet, tracking_valid_old_ids,old_id_col, tracking_valid_new_ids,new_id_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Investigating dropped metadata\n",
    "\n",
    "qc = pd.read_csv(\"/Users/nathansuberi/Desktop/RW_Data/Metadata for Upload - QC Ready Metadata.csv\", index_col=[0]).reset_index().set_index(\"Unique ID\")\n",
    "later = pd.read_csv(\"/Users/nathansuberi/Desktop/RW_Data/Metadata for Upload - Metadata for later.csv\", index_col=[0])\n",
    "dropped = pd.read_csv(\"/Users/nathansuberi/Desktop/RW_Data/Metadata for Upload - Updated metadata 12-07.csv\", index_col=[0])\n",
    "\n",
    "dropped_ix = dropped.index\n",
    "later_ix = later.index\n",
    "qc_ix = qc.index\n",
    "\n",
    "qc_in_dropped = [ix for ix in qc_ix if ix in dropped_ix]\n",
    "later_in_dropped = [ix for ix in later_ix if ix in dropped_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(dropped_ix))\n",
    "print(len(later_ix))\n",
    "print(len(qc_ix))\n",
    "print(len(later_in_dropped))\n",
    "print(len(qc_in_dropped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_to_add_to_qc = dropped.drop(qc_in_dropped+later_in_dropped)\n",
    "missing_cols = [col for col in qc.columns if col not in data_to_add_to_qc.columns]\n",
    "missing_cols\n",
    "\n",
    "#for col in missing_cols:\n",
    "#    data_to_add_to_qc[col] = None\n",
    "\n",
    "#### Transfer info from the Tracking sheet to the Metadata sheet\n",
    "id_col = \"API_ID\"\n",
    "dl_from_src_col = \"Download from Source\"\n",
    "dl_from_s3_col = \"Download Data (S3)\"\n",
    "public_title = \"Public Title\"\n",
    "technical_title = \"Technical Title\"\n",
    "distribution_restriction = \"Distribution Restriction\"\n",
    "shared_api = \"Shared API - Do Not Touch These!\"\n",
    "\n",
    "columns_to_xfer = [id_col, \n",
    "                   dl_from_src_col, dl_from_s3_col, \n",
    "                   public_title, technical_title, \n",
    "                   distribution_restriction, shared_api]\n",
    "\n",
    "current_mdata = transfer_columns_between_tables(dropped, data_to_add_to_qc, columns_to_xfer)\n",
    "current_mdata[\"final_ids\"] = current_mdata[id_col]\n",
    "current_mdata = current_mdata[qc.columns]\n",
    "\n",
    "current_mdata.to_csv(\"/Users/nathansuberi/Desktop/RW_Data/Adding_back_in_missed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Download Google Spreadsheets ####\n",
    "\n",
    "# Make a temporary directory, and download files into it\n",
    "# This makes it easy to remove them later\n",
    "!mkdir temp\n",
    "os.chdir(\"temp\")\n",
    "dest = os.getcwd()\n",
    "\n",
    "# July Data Sheet\n",
    "!curl \"https://docs.google.com/spreadsheets/d/1viPOGYIk6RGu7YMoM3BHNVbkWaCZ0JFBOMSNncWvHYk/export?format=tsv\" > tracking_sheet.tsv\n",
    "tracking_sheet = pd.read_csv(dest+\"/tracking_sheet.tsv\", sep=\"\\t\", index_col=[0])\n",
    "\n",
    "# # Metadata to Upload\n",
    "!curl \"https://docs.google.com/spreadsheets/d/1UkABgMlBIinJjITa6WepFAL-8VBkulS0LCbKojRXjVY/export?format=tsv\" > current_metadata.tsv\n",
    "current_mdata = pd.read_csv(open(dest+\"/current_metadata.tsv\", \"r\"), sep=\"\\t\", index_col=[0])\n",
    "\n",
    "# Delete temporary files\n",
    "os.chdir(\"..\")\n",
    "!rm -r temp\n",
    "\n",
    "\n",
    "#### Check for entries that will break the transfer\n",
    "unmatched = investigate_unmatched(tracking_sheet, current_mdata)\n",
    "print(\"These cause an error: \" + str(unmatched))\n",
    "try:\n",
    "    _, drop_ixs = zip(*unmatched)\n",
    "except:\n",
    "    drop_ixs=[]\n",
    "\n",
    "# Remove the metadata that is not in the tracking sheet, set aside\n",
    "# Need to cast as a list, or else assumes the second item in the list is a column identifier\n",
    "metadata_for_later = current_mdata.loc[list(drop_ixs)]\n",
    "metadata_for_later.to_csv(\"/Users/nathansuberi/Desktop/RW_Data/metadata_for_later.csv\")\n",
    "\n",
    "# Continue with the metadata that matches elements in the tracking sheet\n",
    "current_mdata = current_mdata.drop(list(drop_ixs), axis=0)\n",
    "\n",
    "#### Transfer info from the Tracking sheet to the Metadata sheet\n",
    "# id_col = \"API_ID\"\n",
    "# dl_from_src_col = \"Download from Source\"\n",
    "# dl_from_s3_col = \"Download Data (S3)\"\n",
    "# public_title = \"Public Title\"\n",
    "# technical_title = \"Technical Title\"\n",
    "# distribution_restriction = \"Distribution Restriction\"\n",
    "# shared_api = \"Shared API - Do Not Touch These!\"\n",
    "\n",
    "# columns_to_xfer = [id_col, \n",
    "#                    dl_from_src_col, dl_from_s3_col, \n",
    "#                    public_title, technical_title, \n",
    "#                    distribution_restriction, shared_api]\n",
    "\n",
    "# current_mdata = transfer_columns_between_tables(tracking_sheet, current_mdata, columns_to_xfer)\n",
    "\n",
    "# Set the index of the current_mdata df to be the final id's, \n",
    "# Only move forward with metadata \n",
    "#current_mdata[\"final_ids\"] = current_mdata[id_col]\n",
    "ids_on_backoffice = pd.notnull(current_mdata[\"final_ids\"])\n",
    "mdata_for_ids_on_backoffice = current_mdata.loc[ids_on_backoffice]\n",
    "\n",
    "# Should have used this:\n",
    "mdata_for_ids_on_backoffice = mdata_for_ids_on_backoffice.reset_index().set_index(\"final_ids\")\n",
    "\n",
    "# Save the updated metadata\n",
    "#mdata_for_ids_on_backoffice.to_csv(\"/Users/nathansuberi/Desktop/RW_Data/mdata_for_ids_on_backoffice.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdata_for_ids_on_backoffice.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transfer the subtitles from the metadata sheet to the tracking sheet\n",
    "tracking_sheet_with_subtitles = transfer_columns_between_tables(current_mdata, tracking_sheet, \"Subtitle\")\n",
    "tracking_sheet_with_subtitles.to_csv(\"/Users/nathansuberi/Desktop/RW_Data/tracking_sheet_with_subtitles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(mdata_for_ids_on_backoffice.shape)\n",
    "print(tracking_sheet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(mdata_for_ids_on_backoffice.columns)\n",
    "print(mdata_for_ids_on_backoffice.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## FOR EACH DATASET IN BACKOFFICE THAT HAS METADATA, UPLOAD IT\n",
    "\n",
    "### THIS ADDS ALL DATASETS FOR WHICH WE HAVE METADATA in METADATA FOR UPLOAD ###\n",
    "\n",
    "processed1 = []\n",
    "\n",
    "for rw_id in mdata_for_ids_on_backoffice.index:\n",
    "    url = \"https://api.resourcewatch.org/v1/dataset/\"+str(rw_id)+\"/metadata\"\n",
    "    print(url)\n",
    "    metadata = mdata_for_ids_on_backoffice.loc[rw_id]\n",
    "    \n",
    "    # If there are multiple metadata entries for a single rw_id, print these for trouble-shooting\n",
    "    if len(metadata.shape) > 1:\n",
    "        print(metadata)\n",
    "\n",
    "    # If the data is of type raster, don't include the Download Data (S3) link\n",
    "    flag1 = clean_nulls(metadata[\"Data Type\"]) != None\n",
    "    if(flag1):\n",
    "        flag2 = clean_nulls(metadata[\"Data Type\"]).lower() != \"raster\"\n",
    "        if(flag2):\n",
    "            data_dl_link = clean_nulls(metadata[\"Download Data (S3)\"])\n",
    "        else:\n",
    "            data_dl_link = None\n",
    "    else:\n",
    "        data_dl_link = None\n",
    "\n",
    "    # If there is no download from source, default to the learn more link\n",
    "    if(clean_nulls(metadata[\"Download from Source\"]) != None):\n",
    "        data_dl_orig_link = clean_nulls(metadata[\"Download from Source\"])\n",
    "    else:\n",
    "        data_dl_orig_link = clean_nulls(metadata[\"Learn More Link\"])\n",
    "\n",
    "    # If there is no technical title, default to the public title\n",
    "    if(clean_nulls(metadata[\"Technical Title\"]) != None):\n",
    "        tech_title = clean_nulls(metadata[\"Technical Title\"])\n",
    "    else:\n",
    "        tech_title = clean_nulls(metadata[\"Public Title\"])\n",
    "\n",
    "    print(clean_nulls(metadata[\"Unique ID\"]))\n",
    "\n",
    "    row_payload = {\n",
    "        \"language\": \"en\",\n",
    "\n",
    "        \"name\": clean_nulls(metadata[\"Public Title\"]),\n",
    "        \"description\": clean_nulls(metadata[\"Description\"]),\n",
    "        \"subtitle\": clean_nulls(metadata[\"Subtitle\"]),\n",
    "        \"source\": clean_nulls(metadata[\"Subtitle\"]),\n",
    "        \"functions\": clean_nulls(metadata[\"Function\"]),\n",
    "\n",
    "        \"application\":\"rw\",\n",
    "        \"dataset\":rw_id,\n",
    "\n",
    "        \"info\": {\n",
    "\n",
    "            # One of these a duplicate, test how shows up in front-end\n",
    "            # or should rwId be dataset, above?\n",
    "            \"wri_rw_id\": clean_nulls(metadata[\"Unique ID\"]),\n",
    "            \"rwId\": clean_nulls(metadata[\"Unique ID\"]),\n",
    "\n",
    "            \"data_type\": clean_nulls(metadata[\"Data Type\"]),\n",
    "\n",
    "            \"name\": clean_nulls(metadata[\"Public Title\"]),\n",
    "            \"sources\": create_source_object(clean_nulls(metadata[\"Source Organizations\"])),\n",
    "\n",
    "            \"technical_title\":tech_title,\n",
    "\n",
    "            \"functions\": clean_nulls(metadata[\"Function\"]),\n",
    "            \"cautions\": clean_nulls(metadata[\"Cautions\"]),\n",
    "\n",
    "            \"citation\": clean_nulls(metadata[\"Citation\"]),\n",
    "\n",
    "            \"license\": clean_nulls(metadata[\"Summary of Licence\"]),\n",
    "            \"license_link\": clean_nulls(metadata[\"Link to License\"]),\n",
    "\n",
    "            \"geographic_coverage\": clean_nulls(metadata[\"Geographic Coverage\"]),\n",
    "            \"spatial_resolution\": clean_nulls(metadata[\"Spatial Resolution\"]),\n",
    "\n",
    "            \"date_of_content\": clean_nulls(metadata[\"Date of Content\"]),\n",
    "            \"frequency_of_updates\": clean_nulls(metadata[\"Frequency of Updates\"]),\n",
    "\n",
    "            \"learn_more_link\": clean_nulls(metadata[\"Learn More Link\"]),\n",
    "\n",
    "            \"data_download_link\": data_dl_link,\n",
    "            \"data_download_original_link\":data_dl_orig_link\n",
    "\n",
    "        }\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'content-type': \"application/json\",\n",
    "        'authorization': \"Bearer \" + auth_token,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        processed1.append(rw_id)\n",
    "        res = req.request(\"POST\", url, data=json.dumps(row_payload), headers = headers)\n",
    "        print(res)\n",
    "        if(\"already exists\" in res.text):\n",
    "            res = req.request(\"PATCH\", url, data=json.dumps(row_payload), headers = headers)\n",
    "            print(res)\n",
    "            if(\"errors:\" in res.text):\n",
    "                print(res.text)\n",
    "        elif(\"errors:\" in res.text):\n",
    "            print(res.text)\n",
    "\n",
    "    except TypeError as e:\n",
    "        print(e.args)\n",
    "        print(metadata[[\"Unique ID\", \"Public Title\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Merge subtitles with Tracking sheet\n",
    "\n",
    "#### UPLOADS TITLE, SUBTITLE, AND DOWNLOAD LINKS, if there is no METADATA IN METADATA FOR UPLOAD AVAILABLE (RW_UNIQUE ID EXISTS) ####\n",
    "\n",
    "# Keep only those datasets from trakcing sheet with rw_ids already\n",
    "\n",
    "missed_ids = [rw_id for rw_id in tracking_sheet[\"final_ids\"].values if ((rw_id not in processed1) and (rw_id != None))]\n",
    "\n",
    "missed_data = tracking_sheet.reset_index().set_index(\"final_ids\")\n",
    "missed_data = missed_data.loc[missed_ids]\n",
    "missed_data\n",
    "\n",
    "### THIS ADDS ALL DATASETS FOR WHICH WE HAVE ENTRIES IN TRACKING SHEET and NOTHING IN METADATA FOR UPLOAD###\n",
    "print(\"True if below print empty list []\")\n",
    "print([ind for ind in missed_data.index if ind in mdata_for_ids_on_backoffice.index])\n",
    "\n",
    "processed2 = []\n",
    "\n",
    "for rw_id in missed_data.index:\n",
    "    url = \"https://api.resourcewatch.org/v1/dataset/\"+str(rw_id)+\"/metadata\"\n",
    "    # Everything from current_mdata\n",
    "    metadata = missed_data.loc[rw_id]\n",
    "    #print(metadata)\n",
    "    print(metadata[\"WRI Unique ID\"])\n",
    "    print(metadata[\"Public Title\"])\n",
    "    print(url)\n",
    "    #print(metadata)\n",
    "    row_payload = {\n",
    "        \"language\": \"en\",\n",
    "        \n",
    "        \"name\": clean_nulls(metadata[\"Public Title\"]),\n",
    "        \"subtitle\": clean_nulls(metadata[\"Subtitle\"]),\n",
    "        \n",
    "        \"application\":\"rw\",\n",
    "        \"dataset\":rw_id,\n",
    "        \n",
    "        \"info\": {\n",
    "            \n",
    "            \"wri_rw_id\": clean_nulls(metadata[\"WRI Unique ID\"]),\n",
    "\n",
    "            \"name\": clean_nulls(metadata[\"Public Title\"]),\n",
    "            \"technical_title\":clean_nulls(metadata[\"Technical Title\"]),\n",
    "\n",
    "            \"data_download_link\": clean_nulls(metadata[\"Download Data (S3)\"]), \n",
    "            \"data_download_original_link\": clean_nulls(metadata[\"Download from Source\"])\n",
    "            \n",
    "        }\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'content-type': \"application/json\",\n",
    "        'authorization': \"Bearer \" + auth_token,\n",
    "    }\n",
    "    #print(row_payload)\n",
    "\n",
    "    try:\n",
    "        processed2.append(rw_id)\n",
    "        res = req.request(\"POST\", url, data=json.dumps(row_payload), headers = headers)\n",
    "        if(\"already exists\" in res.text):\n",
    "            res = req.request(\"PATCH\", url, data=json.dumps(row_payload), headers = headers)\n",
    "            if(\"errors:\" in res.text):\n",
    "                print(res.text)\n",
    "        elif(\"errors:\" in res.text):\n",
    "            print(res.text)\n",
    "    except TypeError as e:\n",
    "        print(e.args)\n",
    "        print(metadata[[\"Unique ID\", \"Public Title\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missed_data.to_csv(\"/Users/nathansuberi/Desktop/datasets_on_july_sheet_with_rw_id_no_metadata.csv\")\n",
    "missed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Which datasets do we have metadata for, but are not on tracking sheet? ####\n",
    "# processed1 stores datasets with metadata that were uploaded\n",
    "# current_mdata is all mdata\n",
    "# matched_mdata is all mdata with a final_id\n",
    "print(mdata_for_ids_on_backoffice[\"Unique ID\"].head())\n",
    "print(current_mdata[\"Unique ID\"].head())\n",
    "\n",
    "unmatched_ids = [wri_id for wri_id in current_mdata[\"Unique ID\"].values if wri_id not in mdata_for_ids_on_backoffice[\"Unique ID\"].values]\n",
    "unmatched_mdata = current_mdata.set_index('Unique ID').loc[unmatched_ids]\n",
    "unmatched_mdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access all metadata on backoffice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Base URL for getting dataset metadata from RW API\n",
    "# Metadata = Data that describes Data \n",
    "url = \"https://api.resourcewatch.org/v1/dataset?sort=slug,-provider,userId&status=saved&includes=metadata,vocabulary,widget,layer\"\n",
    "\n",
    "# page[size] tells the API the maximum number of results to send back\n",
    "# There are currently between 200 and 300 datasets on the RW API\n",
    "payload = { \"application\":\"rw\", \"page[size]\": 1000}\n",
    "\n",
    "# Request all datasets, and extract the data from the response\n",
    "res = req.get(url, params=payload)\n",
    "api_response = res.json()[\"data\"]\n",
    "\n",
    "pprint(api_response[0], depth=2)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "### Convert the json object returned by the API into a pandas DataFrame\n",
    "# Another option: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html\n",
    "datasets_on_api = {}\n",
    "\n",
    "for ix, dset in enumerate(api_response):\n",
    "\n",
    "    atts = dset[\"attributes\"]\n",
    "    metadata = atts[\"metadata\"]\n",
    "    layers = atts[\"layer\"]\n",
    "    widgets = atts[\"widget\"]\n",
    "    tags = atts[\"vocabulary\"]\n",
    "    datasets_on_api[atts[\"name\"]] = {\n",
    "        \"rw_id\":dset[\"id\"],\n",
    "        \"upload_name\":atts[\"name\"],\n",
    "        \"table_name\":atts[\"tableName\"],\n",
    "        \"provider\":atts[\"provider\"],\n",
    "        \"date_updated\":atts[\"updatedAt\"],\n",
    "        \"num_metadata_keys\":len(metadata),\n",
    "        \"metadata\": metadata,\n",
    "        \"num_layers\":len(layers),\n",
    "        \"layers\": layers,\n",
    "        \"num_widgets\":len(widgets),\n",
    "        \"widgets\": widgets,\n",
    "        \"num_tags\":len(tags),\n",
    "        \"tags\":tags\n",
    "    }    \n",
    "    \n",
    "# Create the DataFrame, name the index, and sort by date_updated\n",
    "# More recently updated datasets at the top\n",
    "datasets_on_api = pd.DataFrame.from_dict(datasets_on_api, orient='index')\n",
    "\n",
    "def check_public_title(metadata):\n",
    "    if len(metadata) > 0:\n",
    "        mdata = metadata[0]\n",
    "        if \"attributes\" in mdata:\n",
    "            if \"info\" in mdata[\"attributes\"]:\n",
    "                if \"name\" in mdata[\"attributes\"][\"info\"]:\n",
    "                    return(mdata[\"attributes\"][\"info\"][\"name\"])\n",
    "        return(None)\n",
    "\n",
    "# Grab public title, if it exists in metadata\n",
    "datasets_on_api[\"public_title\"] = datasets_on_api.apply(lambda row: check_public_title(row[\"metadata\"]), axis=1)\n",
    "\n",
    "datasets_on_api.set_index(\"rw_id\", inplace=True)\n",
    "datasets_on_api.index.rename(\"Dataset\", inplace=True)\n",
    "datasets_on_api.sort_values(by=[\"date_updated\"], inplace=True, ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull down all the table names - use to make sure this information is up to date on the tracking docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table_names_new = {\"final_ids\":[], \"table_name\":[], \"provider\":[]}\n",
    "\n",
    "for dset in datasets_on_api:    \n",
    "    table_names_new[\"final_ids\"].append(dset[\"id\"])\n",
    "    table_names_new[\"table_name\"].append(dset[\"attributes\"][\"tableName\"])\n",
    "    table_names_new[\"provider\"].append(dset[\"attributes\"][\"provider\"])\n",
    "    \n",
    "dataset_table_names = pd.DataFrame.from_dict(table_names_new).set_index(\"final_ids\")\n",
    "dataset_table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Two methods of merging:\n",
    "\n",
    "# Method 1\n",
    "matching_with_tracking = pd.merge(july_data_upload, dataset_table_names, \n",
    "                                  left_on=\"final_ids\",\n",
    "                                  right_index=True,\n",
    "                                  how=\"left\")\n",
    "matching_with_tracking = matching_with_tracking[[\"final_ids\", \"provider\", \"table_name_y\"]]\n",
    "matching_with_tracking[\"Perfect Dataset?\"] = valid_new_ids_tracking\n",
    "matching_with_tracking.to_csv(\"/Users/nathansuberi/Desktop/RW_Data/update_final_ids.csv\")\n",
    "\n",
    "# Method 2\n",
    "df = july_data_upload.reset_index().merge(dataset_table_names, how=\"left\", on=\"Dataset on Backoffice\").set_index(\"WRI Unique ID\")\n",
    "df.to_csv(\"/Users/nathansuberi/Desktop/RW_Data/tracking_sheet_w_table_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### THIS COVERS ALL DATASETS WHICH ARE ON THE BACKOFFICE but HAVE NO WRI_ID / RW_ID IN TRACKING SHEET ###\n",
    "### Occasionally this is because the data has been moved to after launch\n",
    "\n",
    "### Check if any metadata are not updating as expected ###\n",
    "### Indicating that their unique IDs are wrong in the tracking sheet ###\n",
    "\n",
    "investigate_mdata = current_datasets_on_api[[\"upload_name\", \"public_title\", \"metadata\"]]\n",
    "\n",
    "missed_ids = [rw_id for rw_id in investigate_mdata.index if ((rw_id not in processed1) & (rw_id not in processed2))]\n",
    "\n",
    "investigate_mdata = investigate_mdata.loc[missed_ids]\n",
    "\n",
    "investigate_mdata.to_csv(\"Datasets_on_backoffice_with_no_WRIID.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "investigate_mdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these are datasets for which the Unique ID changed\n",
    "\n",
    "soc.003 Distribution of Infant Mortality\n",
    "soc.016 Conflict and Protest Events in African...\n",
    "dis_007 Landslide Susceptibility Map\n",
    "bio.035 Coral Bleaching Frequency Prediction\n",
    "dis.001 Earthquakes Over the Past 30 days\n",
    "Foo_046a Food Footprint in Protein\n",
    "wat.033 Agriculture Water Demand and Depletion\n",
    "soc.062 Internal Displacement\n",
    "soc.061 Rural Poverty\n",
    "soc.042 Percentage of Urban Population with Ac\n",
    "soc.020 GINI Index\n",
    "soc.008 Gross Domestic Product Per Capita (PPP\n",
    "soc.006 Multidimensional Poverty Index\n",
    "soc.004 Human Development Index\n",
    "soc.002 Gender Development Index\n",
    "foo.002 GLDAS Land Water Content from NOAH Lan..\n",
    "com.028 Effect of Agricultural Policies on Com...\n",
    "cit.029 Municipal Waste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DANGER Bug - able to update metadata for a dataset that no longer exists on the API\n",
    "#test upload cit.029:\n",
    "#    broken, old id: 8f14a33e-5a61-47af-b26e-c1fc036932a5\n",
    "#    working, new id: 00abb46f-34e2-4bf7-be30-1fb0b1de022f\n",
    "    \n",
    "url1=\"https://api.resourcewatch.org/v1/dataset/8f14a33e-5a61-47af-b26e-c1fc036932a5/metadata\"    \n",
    "url2=\"https://api.resourcewatch.org/v1/dataset/10337db6-8321-445e-a60b-28fc1e114f29/metadata\"\n",
    "\n",
    "res1a = req.request(\"POST\", url1, data=json.dumps(row_payload), headers = headers)\n",
    "if(\"already exists\" in res1a.text):\n",
    "    res1b = req.request(\"PATCH\", url1, data=json.dumps(row_payload), headers = headers)\n",
    "        \n",
    "res2a = req.request(\"POST\", url2, data=json.dumps(row_payload), headers = headers)\n",
    "if(\"already exists\" in res2a.text):\n",
    "    res2b = req.request(\"PATCH\", url1, data=json.dumps(row_payload), headers = headers)\n",
    "    \n",
    "print(res1b.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
