{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cartoframes\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 200\n",
    "\n",
    "import requests as req\n",
    "import json\n",
    "import boto3\n",
    "from io import BytesIO, StringIO\n",
    "from gzip import GzipFile\n",
    "import gzip\n",
    "import boto3\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "logging.basicConfig(stream=sys.stderr, level=logging.DEBUG)\n",
    "import random\n",
    "\n",
    "from functools import reduce\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authenticating to Carto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CARTO_USER = 'wri-rw'#os.environ.get('CARTO_USER')\n",
    "CARTO_KEY = ''#os.environ.get('CARTO_KEY')\n",
    "\n",
    "cc = cartoframes.CartoContext(base_url='https://{}.carto.com/'.format(CARTO_USER),\n",
    "                              api_key=CARTO_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authenticating to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aws_access_key_id = ''#os.environ.get('aws_access_key_id')\n",
    "aws_secret_access_key = ''#os.environ.get('aws_secret_access_key')\n",
    "\n",
    "s3_bucket = \"wri-public-data\"\n",
    "s3_folder = \"resourcewatch/georeffed/\"\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "s3_resource = boto3.resource(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "# Functions for reading and uploading data to/from S3\n",
    "def read_from_S3(bucket, key, index_col=0):\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    df = pd.read_csv(BytesIO(obj['Body'].read()), index_col=[index_col], encoding=\"utf8\")\n",
    "    return(df)\n",
    "\n",
    "# client: https://gist.github.com/veselosky/9427faa38cee75cd8e27\n",
    "# resource: https://codereview.stackexchange.com/questions/107412/convert-zip-to-gzip-and-upload-to-s3-bucket\n",
    "# bucket: https://tobywf.com/2017/06/gzip-compression-for-boto3/\n",
    "def write_to_S3(df, bucket, key):\n",
    "    ### Old way\n",
    "    csv_buffer = StringIO()\n",
    "    # Need to set encoding in Python2... default of 'ascii' fails\n",
    "    df.to_csv(csv_buffer, encoding='utf-8')\n",
    "    s3_resource.Object(bucket, key).put(Body=csv_buffer.getvalue())\n",
    "    \n",
    "    \n",
    "    # Zip the csv before posting to s3\n",
    "    # Encode df as csv\n",
    "#     tmp = 'tmp.zip'\n",
    "#     with gzip.open(tmp, 'w') as gz:\n",
    "#         gz.write(df.to_csv().encode(\"utf-8\")) # convert unicode strings to bytes!\n",
    "#         gz.close()\n",
    "#         s3_resource.Object(bucket, key).put(Body=open(tmp, 'rb'))\n",
    "    \n",
    "    # A GzipFile must wrap a real file or a file-like object. We do not want to\n",
    "    # write to disk, so we use a BytesIO as a buffer.\n",
    "#     gz_body = BytesIO()\n",
    "#     gz = GzipFile(None, 'wb', 9, gz_body)\n",
    "#     gz.write(text_body) \n",
    "#     gz.close()\n",
    "    \n",
    "#     # GzipFile has written the compressed bytes into our gz_body\n",
    "#     s3_client.put_object(\n",
    "#         Bucket=bucket,\n",
    "#         Key=key,  # Note: NO .gz extension!\n",
    "#         ContentType='text/csv',  # the original type\n",
    "#         ContentEncoding='gzip',  # MUST have or browsers will error\n",
    "#         Body=gz_body.getvalue()\n",
    "#     )\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # This will work with posted files: \n",
    "    # with gzip.open('soc_074_employment_in_agriculture.zip', 'rb') as f:\n",
    "    #     data = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from RW API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Base URL for getting dataset metadata from RW API\n",
    "url = \"https://api.resourcewatch.org/v1/dataset?sort=slug,-provider,userId&status=saved&includes=metadata,vocabulary,widget,layer\"\n",
    "\n",
    "# page[size] tells the API the maximum number of results to send back\n",
    "# There are currently between 200 and 300 datasets on the RW API\n",
    "payload = { \"application\":\"rw\", \"page[size]\": 1000}\n",
    "\n",
    "# Request all datasets, and extract the data from the response\n",
    "res = req.get(url, params=payload)\n",
    "data = res.json()[\"data\"]\n",
    "\n",
    "### Convert the json object returned by the API into a pandas DataFrame\n",
    "# Another option: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html\n",
    "datasets_on_api = {}\n",
    "for ix, dset in enumerate(data):\n",
    "    atts = dset[\"attributes\"]\n",
    "    metadata = atts[\"metadata\"]\n",
    "    layers = atts[\"layer\"]\n",
    "    widgets = atts[\"widget\"]\n",
    "    tags = atts[\"vocabulary\"]\n",
    "    datasets_on_api[dset[\"id\"]] = {\n",
    "        \"name\":atts[\"name\"],\n",
    "        \"table_name\":atts[\"tableName\"],\n",
    "        \"provider\":atts[\"provider\"],\n",
    "        \"date_updated\":atts[\"updatedAt\"],\n",
    "        \"num_metadata\":len(metadata),\n",
    "        \"metadata\": metadata,\n",
    "        \"num_layers\":len(layers),\n",
    "        \"layers\": layers,\n",
    "        \"num_widgets\":len(widgets),\n",
    "        \"widgets\": widgets,\n",
    "        \"num_tags\":len(tags),\n",
    "        \"tags\":tags\n",
    "    }\n",
    "\n",
    "# Create the DataFrame, name the index, and sort by date_updated\n",
    "# More recently updated datasets at the top\n",
    "current_datasets_on_api = pd.DataFrame.from_dict(datasets_on_api, orient='index')\n",
    "current_datasets_on_api.index.rename(\"Dataset\", inplace=True)\n",
    "current_datasets_on_api.sort_values(by=[\"date_updated\"], inplace=True, ascending = False)\n",
    "\n",
    "# Select all Carto datasets on the API:\n",
    "provider = \"cartodb\"\n",
    "carto_ids = (current_datasets_on_api[\"provider\"]==provider)\n",
    "carto_data = current_datasets_on_api.loc[carto_ids]\n",
    "\n",
    "logging.info(\"Number of Carto datasets: {}\".format(carto_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load georeferencing config & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "georef = {\n",
    "    'geometry':cc.read('wri_countries_a'),\n",
    "    'aliases':cc.read('country_aliases_extended').drop(['index', 'the_geom'], axis=1),\n",
    "    'known_non_un_isos':cc.read('known_non_un_isos').drop(['index', 'the_geom'], axis=1)\n",
    "}\n",
    "\n",
    "georef['iso_aliases'] = georef['aliases'].drop('alias', axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in data sets info from config file\n",
    "#georef_config = pd.read_csv('/Users/nathansuberi/Desktop/RW_Data/georeferencing_tasks/georef_these.csv')\n",
    "#georef_config = georef_config.set_index('wri_id')\n",
    "#georef_config\n",
    "\n",
    "#### Download Google Spreadsheets ####\n",
    "# Georeference Config\n",
    "!curl \"https://docs.google.com/spreadsheets/d/1S4Zh8V_keiDhqfxlATyC8veb3LtZ5W6uM1dyogOL7f0/export?format=tsv\" > georef_config.tsv\n",
    "georef_config = pd.read_csv(open(\"georef_config.tsv\", \"r\"), sep=\"\\t\", index_col=[0])\n",
    "os.remove(\"georef_config.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "georef_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data sets into memory for processing\n",
    "def load_data(obj, elem):\n",
    "    logging.info('Input: {}'.format(elem))\n",
    "    wri_id = elem[0].strip()\n",
    "    rw_id = elem[1].strip()\n",
    "    try:\n",
    "        table_name = carto_data.loc[rw_id]['table_name']\n",
    "        obj[wri_id] = {\n",
    "            'name':table_name,\n",
    "            'data':cc.read(table_name)\n",
    "        }\n",
    "        logging.info('Table name: {}'.format(obj[wri_id]['name']))\n",
    "        logging.info('Table shape: {}'.format(obj[wri_id]['data'].shape))\n",
    "    except:\n",
    "        obj[wri_id] = 'Unavailable'\n",
    "        logging.info('Unavailable')\n",
    "    return obj\n",
    "\n",
    "data_tables = reduce(load_data, zip(georef_config.index,georef_config['rw_id']), {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring the alias table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = georef['aliases']\n",
    "\n",
    "## Adding all countries from our wri-bounds shapefile to the alias table\n",
    "\n",
    "new_aliases = georef['geometry'][['iso_a3', 'name']].copy()\n",
    "new_aliases['alias'] = new_aliases['name']\n",
    "cols = ['iso' if col=='iso_a3' else col for col in new_aliases.columns]\n",
    "cols = [col.strip() for col in cols]\n",
    "new_aliases.columns = cols\n",
    "\n",
    "logging.info('Existing aliases')\n",
    "logging.info(df.columns)\n",
    "logging.info(df.shape)\n",
    "logging.info('Adding aliases from country table')\n",
    "logging.info(new_aliases.columns)\n",
    "logging.info(new_aliases.shape)\n",
    "\n",
    "df = df.append(new_aliases)\n",
    "\n",
    "# ## Adding in new aliases identified by team\n",
    "# logging.info('Adding aliases from csv')\n",
    "# new_aliases = pd.read_csv(ADDITIONAL_ALIASES, header=0)\n",
    "# new_aliases.columns = ['alias', 'name', 'iso']\n",
    "# logging.info(new_aliases.head(5))\n",
    "\n",
    "#### Download Google Spreadsheets ####\n",
    "# Additional Alias List\n",
    "!curl \"https://docs.google.com/spreadsheets/d/11k_6GbFgtF6eAQ3iAjPzt2KWc2n0SsP5P6g7kqILbkM/export?format=tsv\" > additional_aliases.tsv\n",
    "new_aliases = pd.read_csv(open(\"additional_aliases.tsv\", \"r\"), sep=\"\\t\", index_col=None)\n",
    "new_aliases.columns = ['alias', 'name', 'iso']\n",
    "os.remove(\"additional_aliases.tsv\")\n",
    "\n",
    "df = df.append(new_aliases)\n",
    "\n",
    "# Make all aliases lower case, remove spacing\n",
    "df['alias'] = [str(alias).strip().lower().replace(' ','') for alias in df['alias']]\n",
    "\n",
    "## check / remove duplicates\n",
    "sum(df.duplicated(subset=['alias']))\n",
    "sum(df.duplicated(subset=['name']))\n",
    "sum(df.duplicated(subset=['iso']))\n",
    "\n",
    "try:\n",
    "    df = df.drop('the_geom', axis=1)\n",
    "except:\n",
    "    logging.info('unable to drop the_geom from country alias table')\n",
    "try:\n",
    "    df = df.drop('cartodb_georef_status', axis=1)\n",
    "except:\n",
    "    logging.info('unable to drop cartodb_georef_status from country alias table')\n",
    "try:\n",
    "    df = df.drop('index', axis=1)\n",
    "except:\n",
    "    logging.info('unable to drop index from country alias table')\n",
    "try:\n",
    "    df = df.drop('cartodb_id', axis=1)\n",
    "except:\n",
    "    logging.info('unable to drop cartodb_id from country alias table')\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "    \n",
    "georef['aliases'] = df\n",
    "\n",
    "logging.info('Size of current aliasing table: ' + str(georef['aliases'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LOOK_FOR_ISO='fsm'\n",
    "LOOK_FOR_NAME='korea'\n",
    "georef['aliases']['iso'].fillna('', inplace=True)\n",
    "georef['aliases']['name'].fillna('', inplace=True)\n",
    "logging.info(georef['aliases'].loc[georef['aliases']['iso'].str.lower().str.contains(LOOK_FOR_ISO)])\n",
    "logging.info('')\n",
    "logging.info(georef['aliases'].loc[georef['aliases']['name'].str.lower().str.contains(LOOK_FOR_NAME)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform georeferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### \n",
    "## THERE ARE MULTIPLE MATCHES FOR ISO - need to adjust this to only pick once\n",
    "###\n",
    "\n",
    "## NOTE: This can be replaced by making a separate table to join on ISO\n",
    "# def accept_new(agg, nxt):\n",
    "#     cur = agg['cur']\n",
    "#     if not nxt:\n",
    "#         agg['lastval'] = nxt\n",
    "#         agg['indices'].append(cur)\n",
    "#     else:\n",
    "#         if agg['lastval'] != nxt:\n",
    "#             agg['lastval'] = nxt\n",
    "#             agg['indices'].append(cur)\n",
    "#     agg['cur'] += 1\n",
    "#     return agg\n",
    "    \n",
    "# def clean_repeats(isos):\n",
    "#     vals = isos.values\n",
    "#     seen = []\n",
    "#     agg = reduce(accept_new, vals, {'lastval':None, 'indices':[], 'cur':0})\n",
    "#     ixs = agg['indices']\n",
    "#     logging.info('INDICES: {}'.format(ixs))\n",
    "#     return ixs\n",
    "\n",
    "\n",
    "\n",
    "# Tracking all mis-matched names\n",
    "missed_names = {}\n",
    "missed_isos = {}\n",
    "\n",
    "name_alias_info = georef['aliases']\n",
    "iso_alias_info = georef['iso_aliases']\n",
    "\n",
    "for wri_id, info in data_tables.items():\n",
    "    \n",
    "    logging.info('Processing table ' + wri_id)\n",
    "    if type(info) == str:\n",
    "        logging.info('Unavailable, skipping')\n",
    "        continue\n",
    "   \n",
    "    ### WARNING: non standardized indices in the data cause problems after the merge step\n",
    "    name = info['name']\n",
    "    data = info['data'].copy()\n",
    "    data.index = list(range(data.shape[0]))\n",
    "    logging.info('Table head: {}'.format(data.head(15)))\n",
    "\n",
    "    c_code = georef_config.loc[wri_id, 'country_code']\n",
    "    c_code = None if pd.isnull(c_code) else c_code\n",
    "    c_name = georef_config.loc[wri_id, 'country_name']\n",
    "    c_name = None if pd.isnull(c_name) else c_name\n",
    "    \n",
    "    logging.info('c_code: ***{}***'.format(c_code))\n",
    "    logging.info('c_name: ***{}***'.format(c_name))\n",
    "    \n",
    "    # Check if isos match our table\n",
    "    process_by_name = True\n",
    "    if c_code:\n",
    "        logging.info('already has an iso3 code, in column {}'.format(c_code))\n",
    "        _data = data.copy()\n",
    "        \n",
    "        data_with_alias = _data.merge(iso_alias_info,\n",
    "                           left_on=c_code,\n",
    "                           right_on='iso', \n",
    "                           how='left')\n",
    "        try:\n",
    "            null_isos = pd.isnull(data_with_alias['iso'])\n",
    "        except:\n",
    "            null_isos = pd.isnull(data_with_alias['iso_y'])\n",
    "            \n",
    "        if sum(null_isos):\n",
    "            no_iso_match = data_with_alias[null_isos]\n",
    "            logging.info('no match for these isos in the data being processed: ')\n",
    "            logging.info(no_iso_match[c_code].unique())\n",
    "            try:\n",
    "                logging.info(no_iso_match[c_code].unique())\n",
    "                missed_isos[wri_id] = no_iso_match[c_code].unique()\n",
    "            except:\n",
    "                c_code = c_code+'_x'\n",
    "                logging.info(no_iso_match[c_code].unique())\n",
    "                missed_isos[wri_id] = no_iso_match[c_code].unique()\n",
    "    \n",
    "        ### data IS ALTERED HERE\n",
    "        \n",
    "        logging.info('OG data shape: {}'.format(data.shape))\n",
    "        logging.info('Augmented data shape: {}'.format(data_with_alias.shape))\n",
    "        \n",
    "        try:\n",
    "#             ixs = clean_repeats(data_with_alias['iso'])\n",
    "#             data['rw_country_code'] = pd.Series([val for ix, val in enumerate(data_with_alias['iso'].values) if ix in ixs])\n",
    "            data['rw_country_code'] = data_with_alias['iso'].values\n",
    "        except:\n",
    "#             ixs = clean_repeats(data_with_alias['iso_y'])\n",
    "#             data['rw_country_code'] = pd.Series([val for ix, val in enumerate(data_with_alias['iso_y'].values) if ix in ixs])\n",
    "            data['rw_country_code'] = data_with_alias['iso_y'].values\n",
    "        try:\n",
    "#            data['rw_country_name'] = pd.Series([val for ix, val in enumerate(data_with_alias['name'].values) if ix in ixs])\n",
    "            data['rw_country_name'] = data_with_alias['name']\n",
    "        except:\n",
    "#            data['rw_country_name'] = pd.Series([val for ix, val in enumerate(data_with_alias['name_y'].values) if ix in ixs])\n",
    "            data['rw_country_name'] = data_with_alias['name_y']\n",
    "       \n",
    "        try:\n",
    "            data = data.drop('the_geom', axis=1)\n",
    "        except:\n",
    "            logging.info('unable to drop the_geom from {} data'.format(name))\n",
    "            \n",
    "        try:\n",
    "            data = data.drop('cartodb_georef_status', axis=1)\n",
    "        except:\n",
    "            logging.info('unable to drop cartodb_georef_status from {} data'.format(name))\n",
    "\n",
    "        process_by_name = False\n",
    "    \n",
    "    # If country name is supplied, check how many match up with alias/name in country_aliases\n",
    "    if c_name and process_by_name:       \n",
    "        # Ensure that leading or trailing spaces don't break the match\n",
    "        #data[c_name] = ['North Korea' if name=='Korea, Dem. People\\x92s Rep.' else name for name in data[c_name]]\n",
    "        _data = data.copy()\n",
    "        \n",
    "        _data['join_col'] = data[c_name].apply(lambda item: item.strip().lower().replace(' ','').replace('’', '\\''))\n",
    "    \n",
    "        data_with_alias = _data.merge(name_alias_info, \n",
    "                                         left_on = 'join_col',\n",
    "                                         right_on = 'alias',\n",
    "                                         how='left') \n",
    "\n",
    "        null_aliases = pd.isnull(data_with_alias['alias'])             \n",
    "            \n",
    "        logging.info('data with alias df:')\n",
    "        logging.info(data_with_alias.shape)\n",
    "        logging.info(data_with_alias.head(6))\n",
    "        logging.info('raw data')\n",
    "        logging.info(_data.shape)\n",
    "        logging.info(_data.head(5))\n",
    "    \n",
    "        \n",
    "        if sum(null_aliases):\n",
    "            no_alias_match = data_with_alias.loc[null_aliases]\n",
    "            logging.info('missed aliases, matching on column \"alias\" of country_aliases')\n",
    "            logging.info(no_alias_match)\n",
    "            try:\n",
    "                logging.info(no_alias_match[c_name].unique())\n",
    "                missed_names[wri_id] = no_alias_match[c_name].unique()\n",
    "            except:\n",
    "                c_name = c_name+'_x'\n",
    "                logging.info(no_alias_match[c_name].unique())\n",
    "                missed_names[wri_id] = no_alias_match[c_name].unique()\n",
    "                \n",
    "        ### data IS ALTERED HERE\n",
    "\n",
    "        try:\n",
    "            data['rw_country_code'] = data_with_alias['iso']\n",
    "        except:\n",
    "            data['rw_country_code'] = data_with_alias['iso_y']\n",
    "            \n",
    "        try:\n",
    "            data['rw_country_name'] = data_with_alias['name']  \n",
    "        except:\n",
    "            data['rw_country_name'] = data_with_alias['name_y'] \n",
    "            \n",
    "        try:\n",
    "            data = data.drop('the_geom', axis=1)\n",
    "        except:\n",
    "            logging.info('unable to drop the_geom from {} data'.format(name))\n",
    "            \n",
    "        try:\n",
    "            data = data.drop('cartodb_georef_status', axis=1)\n",
    "        except:\n",
    "            logging.info('unable to drop cartodb_georef_status from {} data'.format(name))\n",
    "\n",
    "        \n",
    "    ### SUCCESS\n",
    "    logging.info('Final head:')\n",
    "    logging.info(data.head(5))\n",
    "\n",
    "\n",
    "    data_tables[wri_id]['data'] = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Processed\n",
    "print('Processed')\n",
    "for name, data in data_tables.items():\n",
    "    if not isinstance(data,str):\n",
    "        print(name)\n",
    "        \n",
    "# Not processed\n",
    "print('Not Processed')\n",
    "for name, data in data_tables.items():\n",
    "    if isinstance(data,str):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "NOTE: Need to change layers for these\n",
    "\n",
    "~ done ~ cit.013\n",
    "~ longform ~ cit.020\n",
    "~ longform ~ cli.022\n",
    "~ remove column index? ~ ene.012\n",
    "~ needs a year column ~ for.020\n",
    "~ stored in insights ~ soc.001\n",
    "~ no year col ~ soc.002\n",
    "~ no year col update table name to be soc_012_... instead of soc_12_... ~ soc.012\n",
    "~ no year col, except for a 2016 epi score ~ soc.021\n",
    "~ no year col ~ soc.022\n",
    "~ data refers to multiple years, need to datetime it's year col ~ soc.024\n",
    "~ no year col, 2016 data ~ soc.026\n",
    "~ no year col ~ soc.045\n",
    "~ convert year to datetime ~ soc.055\n",
    "~ no year col ~ soc.067\n",
    "\n",
    "~ shouldn't be georeffed b/c can't lost geometry col ~ com.022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for missed names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(missed_names)\n",
    "print(missed_isos)\n",
    "\n",
    "print('Newly missed names:')\n",
    "for wri_id, names in missed_names.items():\n",
    "    print('Missed names in data set {}'.format(wri_id))\n",
    "    for name in names:\n",
    "        if name not in georef['known_non_un_isos']['name'].values:\n",
    "            print(name)\n",
    "        \n",
    "print('Newly missed isos:')\n",
    "for wri_id, isos in missed_isos.items():\n",
    "    print('Missed isos in data set {}'.format(wri_id))\n",
    "    for iso in isos:\n",
    "        if iso not in georef['known_non_un_isos']['iso'].values:\n",
    "            print(iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Process to investigate misses for a specific dataset\n",
    "# Here, discovered issue with using apostrophe \"’\" instead of \"'\"\n",
    "# Led to augmenting data prep for the georeferencing step above\n",
    "\n",
    "df = data_tables['for.020']\n",
    "df['join_col'] = df['country'].apply(lambda item: item.strip().lower().replace(' ',''))\n",
    "\n",
    "df_a = df.merge(georef['aliases'],\n",
    "                           left_on='join_col',\n",
    "                           right_on='alias', \n",
    "                           how='left')\n",
    "df_a.loc[pd.isnull(df_a['alias']),['country','join_col','alias', 'iso', 'name']]\n",
    "\n",
    "print('democraticpeople’srepublicofkorea' in georef['aliases']['alias'])\n",
    "\n",
    "LOOK_FOR = 'people\\'s'\n",
    "print('Viewing aliases with a name that contains {}:'.format(LOOK_FOR))\n",
    "df = georef['aliases']\n",
    "print(df.loc[df['alias'].str.lower().str.contains(LOOK_FOR)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile known not-included country names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "known_non_un_names_isos = cc.read('known_non_un_isos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(obj, new_list):\n",
    "    obj.extend(new_list)\n",
    "    return obj\n",
    "\n",
    "other_isos = reduce(flatten, missed_isos.values(), [])\n",
    "other_names = reduce(flatten, missed_names.values(), [])\n",
    "\n",
    "def gather_names(iso):\n",
    "    name = input('Official name of {}?'.format(iso))\n",
    "    return((iso, name))\n",
    "\n",
    "def gather_isos(name):\n",
    "    iso = input('Official iso of {}?'.format(name))\n",
    "    return((iso, name))\n",
    "\n",
    "isos_and_names = list(map(gather_names, other_isos))\n",
    "names_and_isos = list(map(gather_isos, other_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isos_and_names = pd.DataFrame()\n",
    "for ds, isos in missed_isos.items():\n",
    "    codecol = georef_config.loc[ds, 'country_code']\n",
    "    namecol = georef_config.loc[ds, 'country_name']\n",
    "    isos_and_names = isos_and_names.append(data_tables[ds]['data'].set_index(codecol).loc[isos, namecol].reset_index())\n",
    "isos_and_names.drop_duplicates(inplace=True)\n",
    "isos_and_names.columns = ['iso', 'name']\n",
    "isos_and_names['reason'] = 'not un single country'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isos_and_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_and_isos = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(isos_and_names)\n",
    "print(names_and_isos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deleted isos\n",
    "deleted_isos = '''AFI French Afar and Issas\n",
    "ATB British Antarctic Territory\n",
    "ATN Dronning Maud Land\n",
    "CTE Canton and Enderbury Islands\n",
    "DDR German Democratic Republic\n",
    "DHY Dahomey\n",
    "GEL Gilbert and Ellice Islands\n",
    "HVO Upper Volta\n",
    "JTN Johnston Island\n",
    "MID Midway Islands\n",
    "NHB New Hebrides\n",
    "PCI Pacific Islands, Trust Territory of the\n",
    "PCZ Panama Canal Zone\n",
    "PHI Philippines – Code changed to PHL\n",
    "PUS U.S. Miscellaneous Pacific Islands\n",
    "RHO Southern Rhodesia\n",
    "SKM Sikkim\n",
    "VDR Viet-Nam, Democratic Republic of\n",
    "WAK Wake Island\n",
    "YMD Yemen, Democratic'''\n",
    "\n",
    "by_line = deleted_isos.split('\\n')\n",
    "by_pair = [(line.split(' ')[0],' '.join(line.split(' ')[1:]) ) for line in by_line]\n",
    "deleted_isos = pd.DataFrame(by_pair)\n",
    "deleted_isos.columns = ['iso', 'name']\n",
    "\n",
    "not_reported_isos = pd.DataFrame(isos_and_names)\n",
    "not_reported_isos.columns = ['iso', 'name']\n",
    "\n",
    "#not_reported_names = pd.DataFrame(names_and_isos)\n",
    "#not_reported_names.columns = ['iso', 'name']\n",
    "#not_reported_names = not_reported_names.drop([0, 7, 19, 32])\n",
    "\n",
    "deleted_isos['reason'] = 'deleted'\n",
    "not_reported_isos['reason'] = 'not un single country'\n",
    "not_reported_names['reason'] = 'not un single country'\n",
    "\n",
    "df = deleted_isos.copy()\n",
    "df = df.append(not_reported_isos)\n",
    "df = df.append(not_reported_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "known_non_un_names_isos = known_non_un_names_isos.append(isos_and_names)\n",
    "known_non_un_names_isos\n",
    "cc.write(known_non_un_names_isos, 'known_non_un_isos', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading finished files to Carto and S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ds, info in data_tables.items():\n",
    "    if type(info) == str:\n",
    "        logging.info('Unavailable, skipping')\n",
    "        continue\n",
    "        \n",
    "    name = info['name']\n",
    "    data = info['data']\n",
    "    print(data.head())\n",
    "    \n",
    "    write_to_S3(data,s3_bucket,s3_folder+name+'.csv')\n",
    "    print('saved ' + name + ' georeffed data to s3')\n",
    "    \n",
    "    cc.write(data, name, overwrite=True)\n",
    "    print('saved ' + name + ' georeffed data to Carto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
