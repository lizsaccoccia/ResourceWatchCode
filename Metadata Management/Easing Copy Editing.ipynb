{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 200\n",
    "\n",
    "import requests as req\n",
    "import json\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "logging.basicConfig(stream=sys.stderr, level=logging.DEBUG)\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RW_API_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab website metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.resourcewatch.org\n",
      "DEBUG:urllib3.connectionpool:https://api.resourcewatch.org:443 \"GET /v1/dataset?sort=slug,-provider,userId&status=saved&published=true&includes=metadata,vocabulary,widget,layer&application=rw&page%5Bsize%5D=1000&language=en HTTP/1.1\" 200 499170\n",
      "INFO:root:Number of datasets: 208\n"
     ]
    }
   ],
   "source": [
    "# Base URL for getting dataset metadata from RW API\n",
    "url = \"https://api.resourcewatch.org/v1/dataset?sort=slug,-provider,userId&status=saved&published=true&includes=metadata,vocabulary,widget,layer\"\n",
    "\n",
    "# page[size] tells the API the maximum number of results to send back\n",
    "# There are currently between 200 and 300 datasets on the RW API\n",
    "payload = { \"application\":\"rw\", \"page[size]\": 1000, \"language\":\"en\"}\n",
    "\n",
    "# Request all datasets, and extract the data from the response\n",
    "res = req.get(url, params=payload)\n",
    "data = res.json()[\"data\"]\n",
    "\n",
    "### Convert the json object returned by the API into a pandas DataFrame\n",
    "# Another option: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html\n",
    "datasets_on_api = {}\n",
    "for ix, dset in enumerate(data):\n",
    "    atts = dset[\"attributes\"]\n",
    "    metadata = atts[\"metadata\"]\n",
    "    layers = atts[\"layer\"]\n",
    "    widgets = atts[\"widget\"]\n",
    "    tags = atts[\"vocabulary\"]\n",
    "    datasets_on_api[dset[\"id\"]] = {\n",
    "        \"name\":atts[\"name\"],\n",
    "        \"table_name\":atts[\"tableName\"],\n",
    "        \"provider\":atts[\"provider\"],\n",
    "        \"date_updated\":atts[\"updatedAt\"],\n",
    "        \"num_metadata\":len(metadata),\n",
    "        \"metadata\": metadata,\n",
    "        \"num_layers\":len(layers),\n",
    "        \"layers\": layers,\n",
    "        \"num_widgets\":len(widgets),\n",
    "        \"widgets\": widgets,\n",
    "        \"num_tags\":len(tags),\n",
    "        \"tags\":tags\n",
    "    }\n",
    "\n",
    "# Create the DataFrame, name the index, and sort by date_updated\n",
    "# More recently updated datasets at the top\n",
    "current_datasets_on_api = pd.DataFrame.from_dict(datasets_on_api, orient='index')\n",
    "current_datasets_on_api.index.rename(\"Dataset\", inplace=True)\n",
    "current_datasets_on_api.sort_values(by=[\"date_updated\"], inplace=True, ascending = False)\n",
    "\n",
    "logging.info(\"Number of datasets: \" + str(current_datasets_on_api.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_datasets_on_api.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull metadata, layer, widget info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pick_layer_info(layer, f):\n",
    "    s_atts = layer['attributes']\n",
    "    t_atts = ['name', 'description']\n",
    "    f.write('*layer id*: {}\\n'.format(layer['id']))\n",
    "    for att in t_atts:\n",
    "        if att in s_atts:\n",
    "            f.write('*{}*: {}\\n'.format(att, s_atts[att]))\n",
    "    f.write('~end_layer~\\n\\n')\n",
    "\n",
    "def pick_widget_info(widget, f):\n",
    "    s_atts = widget['attributes']\n",
    "    t_atts = ['name', 'description']\n",
    "    f.write('*widget id*: {}\\n'.format(widget['id']))\n",
    "    for att in t_atts:\n",
    "        if att in s_atts:\n",
    "            f.write('*{}*: {}\\n'.format(att, s_atts[att]))\n",
    "    f.write('~end_widget~\\n\\n')\n",
    "    \n",
    "def pick_metadata_info(metadata, f):\n",
    "    if not len(metadata):\n",
    "        return\n",
    "\n",
    "    s_atts = metadata['attributes']\n",
    "    t_top_atts = ['description']\n",
    "    t_info_atts = ['cautions', 'functions', 'citation']\n",
    "    f.write('*metadata id*: {}\\n'.format(metadata['id']))\n",
    "    \n",
    "    for att in t_top_atts:\n",
    "        if att in s_atts:\n",
    "            f.write('*{}*: {}\\n'.format(att, s_atts[att]))\n",
    "    \n",
    "    for att in t_info_atts:\n",
    "        if att in s_atts['info']:\n",
    "            f.write('*{}*: {}\\n'.format(att, s_atts['info'][att]))\n",
    "    f.write('~end_metadata~\\n\\n')\n",
    "    \n",
    "def record_dataset_info(info, f):\n",
    "    '''Includes information on metadata, layers, widgets, and tags'''\n",
    "    ds = info[0]\n",
    "    info = info[1]\n",
    "    \n",
    "    name = info['name']\n",
    "    metadata = info['metadata']\n",
    "    layers = info['layers']\n",
    "    widgets = info['widgets']\n",
    "    tags = info['tags']\n",
    "    \n",
    "    f.write('** NEW DATASET **\\n')\n",
    "    f.write('ds name: {}\\n'.format(name))\n",
    "    f.write('ds id: {}\\n'.format(ds))\n",
    "    f.write('ds on site: https://staging.resourcewatch.org/data/explore/{}\\n\\n'.format(ds))\n",
    "    \n",
    "    f.write('~ metadata ~\\n\\n')\n",
    "    list(map(lambda m: pick_metadata_info(m, f), metadata))\n",
    "    \n",
    "    f.write('~ layers ~\\n\\n')\n",
    "    list(map(lambda l: pick_layer_info(l,f),layers))\n",
    "    \n",
    "    f.write('~ widgets ~\\n\\n')\n",
    "    list(map(lambda w: pick_widget_info(w,f),widgets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.unlink('all_info.txt')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "f = open('all_info.txt', 'a')\n",
    "\n",
    "list(map(lambda i: record_dataset_info(i, f), current_datasets_on_api.iterrows()))\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prove inverse works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isplit(iterable,splitters):\n",
    "    return [list(g) for k,g in itertools.groupby(iterable,lambda x:x in splitters) if not k]\n",
    "\n",
    "def patch_layers(info, ds):\n",
    "    \n",
    "    rw_api_url = 'https://api.resourcewatch.org/v1/dataset/{}/layer'.format(ds)\n",
    "    old_layers = req.get(rw_api_url).json()['data']\n",
    "    \n",
    "    edit_layers = isplit(info, '~end_layer~')\n",
    "    edit_layer_info = {}\n",
    "    for edit_layer in edit_layers:\n",
    "        e_id = edit_layer[0].split(':')[1].strip()\n",
    "        edit_layer_info[e_id] = {\n",
    "            'name':edit_layer[1].split(':')[1].strip(),\n",
    "            'description':edit_layer[2].split(':')[1].strip()\n",
    "        }\n",
    "        \n",
    "    for old_layer in old_layers:\n",
    "        layer_id = old_layer['id']\n",
    "        if layer_id in edit_layer_info:\n",
    "            logging.info('Layer ID: {}'.format(layer_id))\n",
    "\n",
    "            new_layer = old_layer.copy()\n",
    "            new_atts = old_layer['attributes'].copy()\n",
    "            edit_atts = edit_layer_info[layer_id]\n",
    "            \n",
    "            new_atts.update(name = edit_atts['name'], description= edit_atts['description'])\n",
    "            new_layer.update(attributes = new_atts)\n",
    "            logging.info('New layer: {}'.format(new_layer))\n",
    "            \n",
    "            patch_url = 'https://api.resourcewatch.org/v1/dataset/{}/layer/{}'.format(ds, layer_id)\n",
    "            headers = {\n",
    "                'content-type': \"application/json\",\n",
    "                'authorization': \"Bearer {}\".format(RW_API_TOKEN)\n",
    "            }\n",
    "            logging.debug('URL: {}'.format(patch_url))\n",
    "            # Tried json.dumps(layer) and that didn't work\n",
    "            res = req.request(\"PATCH\", patch_url, data=json.dumps(new_atts), headers = headers)\n",
    "            logging.info('Layer load Response: {}'.format(res.text))\n",
    "    \n",
    "def patch_widgets(info, ds):\n",
    "    \n",
    "    rw_api_url = 'https://api.resourcewatch.org/v1/dataset/{}/widget'.format(ds)\n",
    "    old_widgets = req.get(rw_api_url).json()['data']\n",
    "    \n",
    "    edit_widgets = isplit(info, '~end_widget~')\n",
    "    edit_widget_info = {}\n",
    "    for edit_widget in edit_widgets:\n",
    "        e_id = edit_widget[0].split(':')[1].strip()\n",
    "        edit_widget_info[e_id] = {\n",
    "            'name':edit_widget[1].split(':')[1].strip(),\n",
    "            'description':edit_widget[2].split(':')[1].strip()\n",
    "        }\n",
    "        \n",
    "    for old_widget in old_widgets:\n",
    "        widget_id = old_widget['id']\n",
    "        if widget_id in edit_widget_info:\n",
    "            logging.info('Widget ID: {}'.format(widget_id))\n",
    "\n",
    "            new_widget = old_widget.copy()\n",
    "            new_atts = old_widget['attributes'].copy()\n",
    "            edit_atts = edit_widget_info[widget_id]\n",
    "            \n",
    "            new_atts.update(name = edit_atts['name'], description= edit_atts['description'])\n",
    "            new_widget.update(attributes = new_atts)\n",
    "            logging.info('New widget: {}'.format(new_widget))\n",
    "            \n",
    "            patch_url = 'https://api.resourcewatch.org/v1/dataset/{}/widget/{}'.format(ds, widget_id)\n",
    "            headers = {\n",
    "                'content-type': \"application/json\",\n",
    "                'authorization': \"Bearer {}\".format(RW_API_TOKEN)\n",
    "            }\n",
    "            logging.debug('URL: {}'.format(patch_url))\n",
    "            # Tried json.dumps(layer) and that didn't work\n",
    "            res = req.request(\"PATCH\", patch_url, data=json.dumps(new_atts), headers = headers)\n",
    "            logging.info('Widget load Response: {}'.format(res.text))\n",
    "    \n",
    "def patch_metadata(info, ds):\n",
    "    rw_api_url = 'https://api.resourcewatch.org/v1/dataset/{}/metadata'.format(ds)\n",
    "    old_metadatas = req.get(rw_api_url).json()['data']\n",
    "    \n",
    "    edit_metadatas = isplit(info, '~end_metadata~')\n",
    "    edit_metadata_info = {}\n",
    "    for edit_metadata in edit_metadatas:\n",
    "        e_id = edit_metadata[0].split(':')[1].strip()\n",
    "        edit_metadata_info[e_id] = {\n",
    "            'description':edit_metadata[1].split(':')[1].strip(),\n",
    "            'cautions':edit_metadata[2].split(':')[1].strip(),\n",
    "            'functions':edit_metadata[3].split(':')[1].strip(),\n",
    "            'citation':edit_metadata[4].split(':')[1].strip()\n",
    "        }\n",
    "        \n",
    "    for old_metadata in old_metadatas:\n",
    "        metadata_id = old_metadata['id']\n",
    "        if metadata_id in edit_metadata_info:\n",
    "            logging.info('Metadata ID: {}'.format(metadata_id))\n",
    "\n",
    "            new_metadata = old_metadata.copy()\n",
    "            new_atts = old_metadata['attributes'].copy()\n",
    "            new_info = new_atts['info'].copy()\n",
    "            edit_atts = edit_metadata_info[metadata_id]\n",
    "            \n",
    "            new_atts.update(description= edit_atts['description'])\n",
    "            new_info.update(cautions = edit_atts['cautions'], functions = edit_atts['functions'])\n",
    "            new_atts.update(info = new_info)\n",
    "            \n",
    "            new_metadata.update(attributes = new_atts)\n",
    "            logging.info('New metadata: {}'.format(new_metadata))\n",
    "            \n",
    "            patch_url = 'https://api.resourcewatch.org/v1/dataset/{}/metadata/{}'.format(ds, metadata_id)\n",
    "            headers = {\n",
    "                'content-type': \"application/json\",\n",
    "                'authorization': \"Bearer {}\".format(RW_API_TOKEN)\n",
    "            }\n",
    "            logging.debug('URL: {}'.format(patch_url))\n",
    "            # Tried json.dumps(layer) and that didn't work\n",
    "            res = req.request(\"PATCH\", patch_url, data=json.dumps(new_atts), headers = headers)\n",
    "            logging.info('Metadata load Response: {}'.format(res.text))\n",
    "    \n",
    "\n",
    "def structure_patchs(lst):\n",
    "    if not len(lst):\n",
    "        return None\n",
    "    \n",
    "    print(lst)\n",
    "    ds = lst[1].split(':')[1].strip()\n",
    "    print(ds)\n",
    "    \n",
    "    metadata_start = lst.index('~ metadata ~')\n",
    "    layers_start = lst.index('~ layers ~')\n",
    "    widgets_start = lst.index('~ widgets ~')\n",
    "    \n",
    "    metadata = lst[metadata_start:layers_start]\n",
    "    layers = lst[layers_start:widgets_start]\n",
    "    widgets = lst[widgets_start:]\n",
    "    \n",
    "    patch_metadata(metadata[1:], ds)\n",
    "    patch_layers(layers[1:], ds)\n",
    "    patch_widgets(widgets[1:], ds)\n",
    "\n",
    "def send_patches(txt):\n",
    "    by_ds = txt.split('** NEW DATASET **') \n",
    "    by_line = list(map(lambda i: i.split('\\n'), by_ds))\n",
    "    clean_lines = list(map(lambda l: list(filter(lambda i: len(i)>0,l)), by_line))\n",
    "    patches = list(map(structure_patchs, clean_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('all_info.txt', 'r') as src:\n",
    "    load_info = send_patches(src.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Experimentation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_info_sample = '** NEW DATASET **\\nds name: soc.069 Political Violence Risk Assessment\\nds id: 25eebe25-aaf2-48fc-ab7b-186d7498f393\\nds on site: https://staging.resourcewatch.org/data/explore/25eebe25-aaf2-48fc-ab7b-186d7498f393\\n\\n~ metadata ~\\n\\nmetadata id: 59debf249d46b60010c48aad\\ndescription: CHANGE ME The Political Violence Risk Assessment data set was originally published by the Hague Centre for Strategic Studies. Its purpose is to forcast the onset of large-scale political violence. Countries are evaluated to determine their \"risk\" of experiencing a large-scale political violence event in the next month using structural data (infant mortality, regime type, conflicts in neighboring countries, and state-led discrimination) and automated event data (protests, assaults, and expressions of disapproval) from the previous month, which are are combined in a logistic regression model. The score is normalized between 0 and 100 and indicates the percentile rank of probability of onset. Infant mortality data are from World Development Indicators; other structural data are from the Center for Systemic Peace; event data are from the Global Database of Events, Language, and Tone; and normalization and aggregation were done by the HCSS. Resource Watch shows only a subset of the dataset. For access to the full dataset and additional information, see the Learn More link.\\ncautions: The data page provides no formal cautions.\\nfunctions: Countries most at risk for political violence\\n~end_metadata~\\n\\nmetadata id: 59e125369817d60012ba095f\\ndescription: Both structural data (infant mortality, regime type, conflicts in neighboring countries, and state-led discrimination) is combined with automated event data (protests, assaults, and expressions of disapproval) of the previous month are combined in a logistic regression model to assess the risk of civil war onset in the next month. The score is normalized between 0 and 100 and indicates the percentile rank of probability of onset. Infant mortality data is from WDI, other structural data is from Center for Systemic Peace, event data is from GDELT, normalization and aggregation were done by HCSS.\\ncautions: The data page provides no formal cautions. \\nfunctions: Countries most at risk for political violence\\n~end_metadata~\\n\\n~ layers ~\\n\\nlayer id: 1f8090c0-6594-45e2-b1df-1cbed74f12a8\\nname: Political Violence Risk Assessment\\ndescription: Risk score on a country level indicating the percentile rank of the likelihood of civil war onset during the next 30 days, updated daily. Both structural indicators and dynamic, event-based variables are used in regression.  The higher the value, the more at risk of civil war.\\n~end_layer~\\n\\n~ widgets ~\\n\\nwidget id: 54160dca-ecb3-4d62-b80e-95970e8e834d\\nname: Political Violence Risk Assessment\\ndescription: \\n~end_widget~\\n\\n'\n",
    "send_patchs(load_info_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(map(len, current_datasets_on_api['metadata']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull out all layer names and definitions\n",
    "* Link to data set detail page\n",
    "* For each layer:\n",
    "* * Layer name\n",
    "* * Layer description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.unlink('layer_info.txt')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "### NOTE: use option 'a' to append to a file. 'w' would overwrite\n",
    "f = open('layer_info.txt', 'a')\n",
    "\n",
    "def pick_layer_info(layer, f):\n",
    "    s_atts = layer['attributes']\n",
    "    t_atts = ['name', 'description']\n",
    "    f.write('layer id: {}\\n'.format(layer['id']))\n",
    "    for att in t_atts:\n",
    "        if att in s_atts:\n",
    "            f.write('{}: {}\\n'.format(att, s_atts[att]))\n",
    "    f.write('\\n')\n",
    "            \n",
    "            \n",
    "def record_layers(info, f):\n",
    "    ds = info[0]\n",
    "    layers = info[1]\n",
    "    f.write('** NEW DATASET **\\n')\n",
    "    f.write('ds id: {}\\n'.format(ds))\n",
    "    f.write('ds on site: https://staging.resourcewatch.org/data/explore/{}\\n\\n'.format(ds))\n",
    "    f.write('~ layers ~\\n\\n')\n",
    "    list(map(lambda l: pick_layer_info(l,f), layers))\n",
    "    f.write('\\n')\n",
    "\n",
    "list(map(lambda i: record_layers(i,f), current_datasets_on_api['layers'].items()))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull out all widget names and definitions\n",
    "* Link to data set detail page\n",
    "* For each widget:\n",
    "* * Widget name\n",
    "* * Widget description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.unlink('widget_info.txt')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "f = open('widget_info.txt', 'a')\n",
    "\n",
    "def pick_widget_info(widget, f):\n",
    "    s_atts = widget['attributes']\n",
    "    t_atts = ['name', 'description']\n",
    "    f.write('widget id: {}\\n'.format(widget['id']))\n",
    "    for att in t_atts:\n",
    "        if att in s_atts:\n",
    "            f.write('{}: {}\\n'.format(att, s_atts[att]))\n",
    "    f.write('\\n')\n",
    "            \n",
    "            \n",
    "def record_widgets(ds, info, f):\n",
    "    ds = info[0]\n",
    "    widgets = info[1]\n",
    "    f.write('** NEW DATASET **\\n')\n",
    "    f.write('ds id: {}\\n'.format(ds))\n",
    "    f.write('ds on site: https://staging.resourcewatch.org/data/explore/{}\\n\\n'.format(ds))\n",
    "    f.write('~ widgets ~\\n\\n')\n",
    "    list(map(lambda l: pick_widget_info(l,f), widgets))\n",
    "    f.write('\\n')\n",
    "\n",
    "list(map(lambda i: record_widgets(i,f), current_datasets_on_api['widgets'].items()))\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
