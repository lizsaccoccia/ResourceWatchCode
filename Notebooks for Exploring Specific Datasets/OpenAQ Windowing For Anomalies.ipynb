{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from copy import copy\n",
    "from functools import reduce\n",
    "import cartoframes\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authenticate to Carto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CARTO_USER = 'rw-nrt'#os.environ.get('CARTO_USER')\n",
    "CARTO_KEY = ''#os.environ.get('CARTO_KEY')\n",
    "\n",
    "cc = cartoframes.CartoContext(base_url='https://{}.carto.com/'.format(CARTO_USER),\n",
    "                              api_key=CARTO_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SD_CUTOFF = 3\n",
    "WINDOW_TIME = timedelta(hours = 12)\n",
    "DATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "# Decide whether to keep a day's data from a station\n",
    "# # Want stations to report at least 3 times, covering 75% of day\n",
    "MIN_COVERAGE = 3\n",
    "MIN_SPAN = timedelta(hours = 10)\n",
    "\n",
    "# Want locations which have valid readings on 75% of days in month\n",
    "MIN_LOC_DAYS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull in legit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Not reading in the whole table\n",
    "#pm25_data = cc.read('cit_003a_air_quality_pm25')\n",
    "pm25_data = pd.read_csv('/Users/nathansuberi/Desktop/RW_Data/pm25_data_for_openaq_blog.csv')\n",
    "pm25_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pm25_data['utc'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def window_by_time(history, ix, window_time):\n",
    "    # Load info\n",
    "    dt, ppm, ix = history[ix]\n",
    "    \n",
    "    # Find lower bound:\n",
    "    lower_bd = copy(ix)\n",
    "    if lower_bd > 0:\n",
    "        while(history[lower_bd][0] > dt - window_time) & (lower_bd > 0):\n",
    "            lower_bd -= 1\n",
    "    \n",
    "    # Find upper bound:\n",
    "    upper_bd = copy(ix)\n",
    "    while(history[upper_bd][0] < dt + window_time) & (upper_bd < len(history)-1):\n",
    "        upper_bd += 1\n",
    "    \n",
    "    # Return result:\n",
    "    if lower_bd == upper_bd:\n",
    "        return [history[ix]]\n",
    "    else:\n",
    "        return history[lower_bd:upper_bd]\n",
    "\n",
    "def return_datetime(timestamp):\n",
    "    return datetime.strptime(timestamp, DATE_FORMAT)\n",
    "    #return timestamp.to_pydatetime()\n",
    "\n",
    "def extract_vals(tuples):\n",
    "    _, vals, _ = zip(*tuples)\n",
    "    return vals\n",
    "\n",
    "def extract_vals_ids(tuples):\n",
    "    _, vals, ids = zip(*tuples)\n",
    "    return (vals, ids)\n",
    "\n",
    "def id_outliers(info):\n",
    "    window, mean, sd = info\n",
    "    outlier_ids = []\n",
    "    vals, ids = extract_vals_ids(window)\n",
    "    for val, _id in zip(vals, ids):\n",
    "        if np.abs((val-mean)/sd) > SD_CUTOFF:\n",
    "            outlier_ids.append(_id)\n",
    "        else:\n",
    "            pass\n",
    "    return outlier_ids\n",
    "\n",
    "def flatten(lst, elems):\n",
    "    for elem in elems:\n",
    "        if elem not in lst:\n",
    "            lst.append(elem)\n",
    "    return lst\n",
    "\n",
    "def extract_day(utc):\n",
    "    return datetime.strptime(utc, DATE_FORMAT).strftime('%d')\n",
    "\n",
    "\n",
    "###\n",
    "# Iterate over locations, set outlier column vals for each as we go\n",
    "###\n",
    "\n",
    "def mark_flags(df, info, num_locations):\n",
    "    \n",
    "    # Track progress through the reduce function\n",
    "    ix, loc_name = info\n",
    "    print('Loc #{}/{}'.format(ix, num_locations))\n",
    "    print('loc_name: {}'.format(loc_name))\n",
    "    \n",
    "    # Extract information for this location\n",
    "    loc_data = df[df['location'] == loc_name]\n",
    "    print('Number of observations: {}'.format(loc_data.shape[0]))\n",
    "\n",
    "    ###\n",
    "    # Mark outliers\n",
    "    ###\n",
    "    \n",
    "    # Convert datetimes, rezip_to_values\n",
    "    utc_val = loc_data[['utc', 'value']].sort_values(by='utc')\n",
    "    dts = list(map(return_datetime, utc_val['utc']))\n",
    "    rezipped = list(zip(dts, loc_data['value']))\n",
    "    \n",
    "    # Add column of indices to history of values\n",
    "    history = list(zip(*zip(*rezipped), range(len(rezipped))))\n",
    "    \n",
    "    # Create windows over history\n",
    "    windowed_by_time_history = list(map(lambda ix: window_by_time(history, ix, WINDOW_TIME), range(len(history))))\n",
    "        \n",
    "    # Calculate mean and standard deviation for windows\n",
    "    mean_windows = list(map(lambda tuples: np.mean(extract_vals(tuples)), windowed_by_time_history))\n",
    "    sd_windows = list(map(lambda tuples: np.std(extract_vals(tuples)), windowed_by_time_history))\n",
    "    \n",
    "    # Package windowed history along w/ means and stardard deviations\n",
    "    eval_package = list(zip(windowed_by_time_history, mean_windows, sd_windows))\n",
    "    \n",
    "    # Identify outliers\n",
    "    outlier_ids = reduce(flatten, list(map(id_outliers, eval_package)), [])\n",
    "    #print('Num outliers: {}'.format(len(outlier_ids)))\n",
    "    \n",
    "    # Mark outliers in dataframe, return to reduce statement\n",
    "    outlier_ixs = loc_data.iloc[outlier_ids].index\n",
    "    df.loc[outlier_ixs, 'outlier'] = True\n",
    "    #print('Outlier indices: {}'.format(outlier_ixs))\n",
    "    \n",
    "    ###\n",
    "    # Mark poorly represented locations\n",
    "    ###\n",
    "    \n",
    "    # Mark which days this location has adequate coverage for\n",
    "    \n",
    "    loc_data_days = [extract_day(utc) for utc in loc_data['utc']]\n",
    "    loc_data['day'] = loc_data_days\n",
    "    \n",
    "    rejected_days = []\n",
    "    accepted_days = []\n",
    "   \n",
    "    for day in set(loc_data_days):\n",
    "        #print('Day: {}'.format(day))\n",
    "        \n",
    "        # Accept by default if all tests passed\n",
    "        accept = True\n",
    "        \n",
    "        day_of_loc_data = loc_data[loc_data['day']==day]\n",
    "        #print(day_of_loc_data.shape)\n",
    "        \n",
    "        # Reject if less than MIN_COVERAGE observations for station that day\n",
    "        if day_of_loc_data.shape[0] < MIN_COVERAGE:\n",
    "            #print('Not enough reports to count location {} on day {}'.format(loc_name, day))\n",
    "            rejected_days.append(day)\n",
    "            accept = False\n",
    "           \n",
    "        # Reject if less than MIN_SPAN hours of the day covered for station that day\n",
    "        time_range = sorted(day_of_loc_data['utc'])\n",
    "        #print(time_range)\n",
    "        start, end = time_range[0], time_range[len(time_range)-1]\n",
    "        start = return_datetime(start)\n",
    "        end = return_datetime(end)\n",
    "        len_range = end - start\n",
    "        #print('Len range: {}'.format(len_range))\n",
    "        \n",
    "        if len_range < MIN_SPAN:\n",
    "            #print('Not enough coverage of day to count location {} on day {}'.format(loc_name, day))\n",
    "            rejected_days.append(day)\n",
    "            accept = False\n",
    "            \n",
    "        # Otherwise, accept this day at this location\n",
    "        if accept:\n",
    "            accepted_days.append(day)\n",
    "        \n",
    "    # Label days for which we had insufficient coverage\n",
    "    for day in rejected_days:\n",
    "        insuff_data_ix = loc_data.loc[loc_data['day']==day].index\n",
    "        df.loc[insuff_data_ix, 'poor_day_at_station'] = True\n",
    "        \n",
    "    # Mark whether location has adequate coverage in the month\n",
    "    if len(accepted_days) < MIN_LOC_DAYS:\n",
    "        #print('Underrepresented station - only reporting for {}, less than minimum of {} days this month'.format(len(accepted_days), MIN_LOC_DAYS))\n",
    "        df.loc[:, 'poor_station'] = True\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mark observations as outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locations = pm25_data['location'].unique()\n",
    "process_locations = locations#['ES1535A']\n",
    "labelled_data = reduce(lambda df, info: mark_flags(df, info, len(process_locations)), enumerate(process_locations), pm25_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to Carto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cc.write(labelled_data, 'cit_003a_air_quality_pm25_flagged', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelled_data.to_csv('cit_003a_air_quality_pm25_flagged_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Example, previously troublesome location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write data back to Carto\n",
    "albacete_example = labelled_data.loc[labelled_data['city'].str.contains('Albacete')]\n",
    "albacete_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "albacete_example.to_csv('albacete_cleaned_example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Emperimentation below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LEN_SAMPLE = 100\n",
    "WINDOW_SIZE = 4\n",
    "\n",
    "\n",
    "obs = np.random.randn(100)*LEN_SAMPLE\n",
    "dates = [datetime.now() - timedelta(seconds=float(10000*np.random.randn(1))) for _ in range(LEN_SAMPLE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = list(zip(sorted(dates), obs, range(len(obs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def window_by_size(index, window_size, lst):\n",
    "    # Window is entire list\n",
    "    if len(lst) < window_size:\n",
    "        return lst\n",
    "    \n",
    "    # Window collides with beginning of the list\n",
    "    if (index < window_size):\n",
    "        if len(lst) < index + window_size:\n",
    "            return lst\n",
    "        else:\n",
    "            return lst[:index + window_size]\n",
    "\n",
    "    # Window collides with end of the list\n",
    "    if (len(lst) < index + window_size):\n",
    "        return lst[index - window_size:]\n",
    "    \n",
    "    # Entire window in list\n",
    "    return lst[index - window_size : index + window_size]\n",
    "\n",
    "def window_by_time(index, window_time, lst):\n",
    "    obs_time = lst[index][0]\n",
    "    \n",
    "    # Find lower bound:\n",
    "    lower_bd = copy(index)\n",
    "    while(lst[lower_bd][0] > obs_time - window_time) & (lower_bd > 0):\n",
    "        lower_bd -= 1\n",
    "    \n",
    "    # Find upper bound:\n",
    "    upper_bd = copy(index)\n",
    "    while(lst[upper_bd][0] < obs_time + window_time) & (upper_bd < len(lst)-1):\n",
    "        upper_bd += 1\n",
    "    \n",
    "    # Return result:\n",
    "    return lst[lower_bd:upper_bd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "windowed_by_size_history = list(map(lambda ix: window_by_size(ix, WINDOW_SIZE, obs), range(len(obs))))\n",
    "windowed_by_time_history = list(map(lambda ix: window_by_time(ix, WINDOW_TIME, history), range(len(history))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowed_by_time_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_vals(tuples):\n",
    "    _, vals, _ = zip(*tuples)\n",
    "    return vals\n",
    "\n",
    "def extract_vals_ids(tuples):\n",
    "    _, vals, ids = zip(*tuples)\n",
    "    return (vals, ids)\n",
    "\n",
    "mean_windows = list(map(lambda tuples: np.mean(extract_vals(tuples)), windowed_by_time_history))\n",
    "sd_windows = list(map(lambda tuples: np.std(extract_vals(tuples)), windowed_by_time_history))\n",
    "\n",
    "eval_package = list(zip(windowed_by_time_history, mean_windows, sd_windows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def id_outliers(info):\n",
    "    window, mean, sd = info\n",
    "    #response = []\n",
    "    outlier_ids = []\n",
    "    vals, ids = extract_vals_ids(window)\n",
    "    for val, _id in zip(vals, ids):\n",
    "        if np.abs((val-mean)/sd) > CUTOFF:\n",
    "            #response.append('Throw away {}'.format(_id))\n",
    "            outlier_ids.append(_id)\n",
    "        else:\n",
    "            pass\n",
    "            #response.append('Keep {}'.format(_id))\n",
    "    return outlier_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flatten(lst, elems):\n",
    "    for elem in elems:\n",
    "        if elem not in lst:\n",
    "            lst.append(elem)\n",
    "    return lst\n",
    "\n",
    "outlier_ids = reduce(flatten, list(map(id_outliers, eval_package)), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(outlier_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull in legit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pm25_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locations = pm25_data['location'].unique()\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc_zero = locations[0]\n",
    "data_zero = pm25_data[pm25_data['location'] == loc_zero]\n",
    "\n",
    "zero_date_vals = data_zero[['utc', 'value']]\n",
    "\n",
    "def return_datetime(timestamp):\n",
    "    return timestamp.to_pydatetime()\n",
    "dts = list(map(return_datetime, zero_date_vals['utc']))\n",
    "\n",
    "new_vals = list(zip(dts, data_zero['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = list(zip(*zip(*new_vals), range(len(new_vals))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowed_by_time_history = list(map(lambda ix: window_by_time(ix, WINDOW_TIME, history), range(len(history))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_vals(tuples):\n",
    "    _, vals, _ = zip(*tuples)\n",
    "    return vals\n",
    "\n",
    "def extract_vals_ids(tuples):\n",
    "    _, vals, ids = zip(*tuples)\n",
    "    return (vals, ids)\n",
    "\n",
    "mean_windows = list(map(lambda tuples: np.mean(extract_vals(tuples)), windowed_by_time_history))\n",
    "sd_windows = list(map(lambda tuples: np.std(extract_vals(tuples)), windowed_by_time_history))\n",
    "\n",
    "eval_package = list(zip(windowed_by_time_history, mean_windows, sd_windows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def id_outliers(info):\n",
    "    window, mean, sd = info\n",
    "    outlier_ids = []\n",
    "    vals, ids = extract_vals_ids(window)\n",
    "    for val, _id in zip(vals, ids):\n",
    "        if np.abs((val-mean)/sd) > CUTOFF:\n",
    "            outlier_ids.append(_id)\n",
    "        else:\n",
    "            pass\n",
    "    return outlier_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flatten(lst, elems):\n",
    "    for elem in elems:\n",
    "        if elem not in lst:\n",
    "            lst.append(elem)\n",
    "    return lst\n",
    "\n",
    "outlier_ids = reduce(flatten, list(map(id_outliers, eval_package)), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sorted(outlier_ids))\n",
    "print(len(outlier_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pm25_data.loc[pm25_data.iloc[outlier_ids].index, 'outlier'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pm25_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With dataframes the way I tried it is slower\n",
    "\n",
    "# With dataframe -- untested\n",
    "def window_by_time_df(history, ix, window_time):\n",
    "    # Load info\n",
    "    dt, ppm, ix = history.iloc[ix][['dt', 'ppm', 'ix']].values \n",
    "   \n",
    "    # Find lower bound:\n",
    "    lower_bd = copy(ix)\n",
    "    while(history.iloc[lower_bd]['dt'] > dt - window_time) & (lower_bd > 0):\n",
    "        lower_bd -= 1\n",
    "    \n",
    "    # Find upper bound:\n",
    "    upper_bd = copy(ix) \n",
    "    while(history.iloc[upper_bd]['dt'] < dt + window_time) & (upper_bd < history.shape[0]-1):\n",
    "        upper_bd += 1\n",
    "    \n",
    "    # Return result:\n",
    "    return history[lower_bd:upper_bd]\n",
    "\n",
    "def id_outliers_df(info):\n",
    "    window, mean, sd = info\n",
    "    outlier_ids = []\n",
    "    #print(window)\n",
    "    for ppm, ix in window[['ppm', 'ix']].values:\n",
    "        #print('Ppm: {}'.format(ppm))\n",
    "        #print('Ix: {}'.format(ix))\n",
    "        if np.abs((ppm-mean)/sd) > CUTOFF:\n",
    "            outlier_ids.append(ix)\n",
    "        else:\n",
    "            pass\n",
    "    return outlier_ids\n",
    "\n",
    "def mark_outliers_df(df, info, num_locations):\n",
    "    # Track progress through the reduce function\n",
    "    ix, loc_name = info\n",
    "    print('Loc #{}/{}'.format(ix, num_locations))\n",
    "    print('loc_name: {}'.format(loc_name))\n",
    "    \n",
    "    # Extract information for this location\n",
    "    loc_data = df[df['location'] == loc_name]\n",
    "    print('Number of observations: {}'.format(loc_data.shape[0]))\n",
    "\n",
    "    # Convert datetimes, rezip_to_values\n",
    "    utc_val = loc_data[['utc', 'value']]\n",
    "    dts = list(map(return_datetime, utc_val['utc']))\n",
    "    rezipped = list(zip(dts, loc_data['value']))\n",
    "    \n",
    "    # Add column of indices to history of values\n",
    "    history = list(zip(*zip(*rezipped), range(len(rezipped))))\n",
    "    #history = pd.DataFrame({'dt':loc_data['utc'], 'ppm':loc_data['value'], 'ix':range(loc_data.shape[0])})\n",
    "    #print('History: {}'.format(history))\n",
    "    \n",
    "    # Create windows over history\n",
    "    windowed_by_time_history = list(map(lambda ix: window_by_time(history, ix, WINDOW_TIME), range(len(history))))\n",
    "    #windowed_by_time_history = list(map(lambda ix: window_by_time(history, ix, WINDOW_TIME), range(history.shape[0])))\n",
    "    #print('Windows: {}'.format(windowed_by_time_history))\n",
    "        \n",
    "    # Calculate mean and standard deviation for windows\n",
    "    mean_windows = list(map(lambda tuples: np.mean(extract_vals(tuples)), windowed_by_time_history))\n",
    "    sd_windows = list(map(lambda tuples: np.std(extract_vals(tuples)), windowed_by_time_history))\n",
    "    #mean_windows = list(map(lambda window: np.mean(window['ppm']), windowed_by_time_history))\n",
    "    #sd_windows = list(map(lambda window: np.std(window['ppm']), windowed_by_time_history))\n",
    "\n",
    "    # Package windowed history along w/ means and stardard deviations\n",
    "    eval_package = list(zip(windowed_by_time_history, mean_windows, sd_windows))\n",
    "    \n",
    "    # Identify outliers\n",
    "    outlier_ids = reduce(flatten, list(map(id_outliers, eval_package)), [])\n",
    "    print('Num outliers: {}'.format(len(outlier_ids)))\n",
    "    \n",
    "    # Mark outliers in dataframe, return to reduce statement\n",
    "    df.loc[df.iloc[outlier_ids].index, 'outlier'] = True\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc_data = pm25_data[pm25_data['location'] == locations[1892]]\n",
    "\n",
    "utc_val = loc_data[['utc', 'value']]\n",
    "    \n",
    "dts = list(map(return_datetime, utc_val['utc']))\n",
    "\n",
    "new_vals = list(zip(dts, loc_data['value']))\n",
    "\n",
    "history = list(zip(*zip(*new_vals), range(len(new_vals))))\n",
    "\n",
    "windowed_by_time_history = list(map(lambda ix: window_by_time(ix, WINDOW_TIME, history), range(len(history))))\n",
    "\n",
    "mean_windows = list(map(lambda tuples: np.mean(extract_vals(tuples)), windowed_by_time_history))\n",
    "sd_windows = list(map(lambda tuples: np.std(extract_vals(tuples)), windowed_by_time_history))\n",
    "\n",
    "eval_package = list(zip(windowed_by_time_history, mean_windows, sd_windows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
